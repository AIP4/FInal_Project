{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 13223,
     "status": "ok",
     "timestamp": 1733376404928,
     "user": {
      "displayName": "한준호",
      "userId": "12870242508044632368"
     },
     "user_tz": -540
    },
    "id": "eyh9pZnrfP0X"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from utils import set_logger, prepare_data_for_CNN_LSTM\n",
    "from dataset import FinedustCNNLSTMDataset\n",
    "from model import FinedustCNNLSTM\n",
    "from utils import prepare_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import logging\n",
    "from interpolate import simple_interpolate\n",
    "import numpy as np\n",
    "from preprocess import minmax_scaling\n",
    "from train import train, train_cnn_lstm\n",
    "from permutation import compute_cnn_lstm_permutation_importance\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1733376404929,
     "user": {
      "displayName": "한준호",
      "userId": "12870242508044632368"
     },
     "user_tz": -540
    },
    "id": "Lx5HNBDMfP0c"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"epochs\": 500,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_layers\": 2,\n",
    "    \"hidden_size\": 128,\n",
    "    \"window_size\": 24,\n",
    "    \"output_size\": 1,\n",
    "    \"dropout_prob\": 0.2,\n",
    "    \"patience\": 10,\n",
    "    \"out_channels\": 16,\n",
    "    \"kernel_size\": 3,\n",
    "    \"K\": 1,\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iaam3qEXfP0e"
   },
   "source": [
    "# CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 768191,
     "status": "error",
     "timestamp": 1733377174842,
     "user": {
      "displayName": "한준호",
      "userId": "12870242508044632368"
     },
     "user_tz": -540
    },
    "id": "rU8d51tsfP0g",
    "outputId": "0522ba2c-077b-4169-ddec-1577993ad7fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/han07301/ai_project/JH/CNN_LSTM/utils.py:119: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[pm_columns] = df[pm_columns].applymap(lambda x: np.nan if x <= 0 else x)\n",
      "/home/han07301/ai_project/JH/CNN_LSTM/interpolate.py:41: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  data = data.interpolate(method='linear')\n",
      "  0%|          | 0/500 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_pickle.Pickler' object attribute 'persistent_id' is read-only",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m     20\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m FinedustCNNLSTM(config,\n\u001b[1;32m     23\u001b[0m                         in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mpm_columns),\n\u001b[1;32m     24\u001b[0m                         input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mfeature_columns))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 26\u001b[0m total_preds, losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_cnn_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCNN_LSTM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m pred_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(total_preds, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     32\u001b[0m val_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([y \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m val_dataset])\n",
      "File \u001b[0;32m~/ai_project/JH/CNN_LSTM/train.py:204\u001b[0m, in \u001b[0;36mtrain_cnn_lstm\u001b[0;34m(model, train_dataset, valid_dataset, region, model_name, config, device, log_suffix)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_loss:\n\u001b[1;32m    203\u001b[0m     best_loss \u001b[38;5;241m=\u001b[39m val_loss\n\u001b[0;32m--> 204\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mregion\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved with loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    206\u001b[0m     epochs_without_improvement \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/practice/lib/python3.13/site-packages/torch/serialization.py:850\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 850\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/practice/lib/python3.13/site-packages/torch/serialization.py:1087\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1085\u001b[0m data_buf \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[1;32m   1086\u001b[0m pickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mPickler(data_buf, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n\u001b[0;32m-> 1087\u001b[0m \u001b[43mpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersistent_id\u001b[49m \u001b[38;5;241m=\u001b[39m persistent_id\n\u001b[1;32m   1088\u001b[0m pickler\u001b[38;5;241m.\u001b[39mdump(obj)\n\u001b[1;32m   1089\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_pickle.Pickler' object attribute 'persistent_id' is read-only"
     ]
    }
   ],
   "source": [
    "regions = [\"Andong\", \"Seoul\", \"Jeonju\", \"Daegu\", \"Gwangju\"]\n",
    "\n",
    "pred_list, loss_list = [], []\n",
    "\n",
    "for region in regions:\n",
    "  if region == \"Jeonju\":\n",
    "    columns_to_remove = [\"CA_TOT\", \"CA_MID\", \"STN\", \"IR\", \"PA\", \"IX\", \"PS\", \"지점\", \"위도\", \"경도\"]\n",
    "  else:\n",
    "    columns_to_remove = [\"CA_TOT\", \"CA_MID\", \"STN\", \"IR\", \"PA\", \"PS\", \"지점\", \"위도\", \"경도\"]\n",
    "  df = prepare_data_for_CNN_LSTM(region.lower(), columns_to_remove)\n",
    "  df = simple_interpolate(df, method=\"linear\")\n",
    "  df, scaler = minmax_scaling(df)\n",
    "\n",
    "  dataset = FinedustCNNLSTMDataset(df,\n",
    "                                  window_size=config[\"window_size\"],\n",
    "                                  prediction_length=config[\"output_size\"],\n",
    "                                  time_window=3)\n",
    "  train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "  train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "  val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "  model = FinedustCNNLSTM(config,\n",
    "                          in_channels=len(dataset.pm_columns),\n",
    "                          input_size=len(dataset.feature_columns)).to(device)\n",
    "\n",
    "  total_preds, losses = train_cnn_lstm(model,\n",
    "                                      train_dataset,\n",
    "                                      val_dataset,\n",
    "                                      region, \"CNN_LSTM\", config,\n",
    "                                      device)\n",
    "  pred_y = np.concatenate(total_preds, axis=0)\n",
    "  val_y = np.concatenate([y for x, y in val_dataset])\n",
    "\n",
    "  pred_list.append((pred_y, val_y))\n",
    "  loss_list.append(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Andong', 'Seoul', 'Jeonju', 'Daegu']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 351013,
     "status": "ok",
     "timestamp": 1733372285708,
     "user": {
      "displayName": "한준호",
      "userId": "12870242508044632368"
     },
     "user_tz": -540
    },
    "id": "rjZ-K-Ru4pvs",
    "outputId": "6471e0a7-88a6-440a-fdf9-b24a257a35d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sangwon/nlplab_leaderboard2/Junho/JH/LSTM/interpolate.py:41: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  data = data.interpolate(method='linear')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MSE Loss: 6.4699\n",
      "Feature: WD, Permutation Importance: 3.6131\n",
      "Feature: WS, Permutation Importance: 3.5572\n",
      "Feature: TA, Permutation Importance: 4.3083\n",
      "Feature: TD, Permutation Importance: 1.9318\n",
      "Feature: HM, Permutation Importance: 7.6094\n",
      "Feature: PV, Permutation Importance: 4.6644\n",
      "Feature: VS, Permutation Importance: 12.2706\n",
      "Feature: TS, Permutation Importance: 3.0322\n",
      "Feature importances saved to importance/LSTM/Andong.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sangwon/nlplab_leaderboard2/Junho/JH/LSTM/interpolate.py:41: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  data = data.interpolate(method='linear')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MSE Loss: 13.5851\n",
      "Feature: WD, Permutation Importance: 6.2550\n",
      "Feature: WS, Permutation Importance: 3.6059\n",
      "Feature: TA, Permutation Importance: 1.1206\n",
      "Feature: TD, Permutation Importance: 1.1049\n",
      "Feature: HM, Permutation Importance: 5.1800\n",
      "Feature: PV, Permutation Importance: 5.0815\n",
      "Feature: VS, Permutation Importance: 10.2841\n",
      "Feature: TS, Permutation Importance: 1.3686\n",
      "Feature: TE_005, Permutation Importance: 0.7640\n",
      "Feature: TE_01, Permutation Importance: 4.8736\n",
      "Feature: TE_02, Permutation Importance: 5.2185\n",
      "Feature: TE_03, Permutation Importance: 1.7073\n",
      "Feature importances saved to importance/LSTM/Seoul.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sangwon/nlplab_leaderboard2/Junho/JH/LSTM/interpolate.py:41: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  data = data.interpolate(method='linear')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FinedustLSTM:\n\tsize mismatch for lstm.lstm.weight_ih_l0: copying a param with shape torch.Size([1024, 8]) from checkpoint, the shape in current model is torch.Size([512, 8]).\n\tsize mismatch for lstm.lstm.weight_hh_l0: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for lstm.lstm.bias_ih_l0: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm.lstm.bias_hh_l0: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm.lstm.weight_ih_l1: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for lstm.lstm.weight_hh_l1: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for lstm.lstm.bias_ih_l1: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm.lstm.bias_hh_l1: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm.fc1.weight: copying a param with shape torch.Size([768, 256]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for lstm.fc1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for lstm.fc2.weight: copying a param with shape torch.Size([256, 768]) from checkpoint, the shape in current model is torch.Size([128, 384]).\n\tsize mismatch for lstm.fc2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 27\u001b[0m\n\u001b[1;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m FinedustLSTM(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mfeature_columns),\n\u001b[1;32m     22\u001b[0m                             hidden_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     23\u001b[0m                             num_layers\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     24\u001b[0m                             output_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     25\u001b[0m                             dropout_prob\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     26\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/LSTM/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m feature_importances \u001b[38;5;241m=\u001b[39m compute_permutation_importance(model, val_loader, dataset\u001b[38;5;241m.\u001b[39mfeature_columns, device)\n",
      "File \u001b[0;32m~/anaconda3/envs/student/lib/python3.12/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FinedustLSTM:\n\tsize mismatch for lstm.lstm.weight_ih_l0: copying a param with shape torch.Size([1024, 8]) from checkpoint, the shape in current model is torch.Size([512, 8]).\n\tsize mismatch for lstm.lstm.weight_hh_l0: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for lstm.lstm.bias_ih_l0: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm.lstm.bias_hh_l0: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm.lstm.weight_ih_l1: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for lstm.lstm.weight_hh_l1: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for lstm.lstm.bias_ih_l1: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm.lstm.bias_hh_l1: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm.fc1.weight: copying a param with shape torch.Size([768, 256]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for lstm.fc1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for lstm.fc2.weight: copying a param with shape torch.Size([256, 768]) from checkpoint, the shape in current model is torch.Size([128, 384]).\n\tsize mismatch for lstm.fc2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([1, 256]) from checkpoint, the shape in current model is torch.Size([1, 128])."
     ]
    }
   ],
   "source": [
    "model_name = \"CNN-LSTM\"\n",
    "\n",
    "output_dir = f\"importance/{model_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for region in regions:\n",
    "  if region == \"Jeonju\":\n",
    "    columns_to_remove = [\"CA_TOT\", \"CA_MID\", \"STN\", \"IR\", \"PA\", \"IX\", \"PS\", \"지점\", \"위도\", \"경도\"]\n",
    "  else:\n",
    "    columns_to_remove = [\"CA_TOT\", \"CA_MID\", \"STN\", \"IR\", \"PA\", \"PS\", \"지점\", \"위도\", \"경도\"]\n",
    "  df = prepare_data_for_CNN_LSTM(region.lower(), columns_to_remove)\n",
    "  df = simple_interpolate(df, method=\"linear\")\n",
    "  df, scaler = minmax_scaling(df)\n",
    "\n",
    "  dataset = FinedustCNNLSTMDataset(df,\n",
    "                                 window_size=config[\"window_size\"],\n",
    "                                 prediction_length=config[\"output_size\"],\n",
    "                                 time_window=3)\n",
    "  train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "  train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "  val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "  if region == \"Jeonju\":\n",
    "    config[\"hidden_size\"] = 256\n",
    "  else:\n",
    "    config[\"hidden_size\"] = 128\n",
    "  model = FinedustCNNLSTM(config,\n",
    "                          in_channels=len(dataset.pm_columns),\n",
    "                          input_size=len(dataset.feature_columns)).to(device)\n",
    "  save_path = f\"models/{model_name}/{region}.pth\"\n",
    "  model.load_state_dict(torch.load(save_path))\n",
    "  model.to(device)\n",
    "\n",
    "  feature_importances = compute_cnn_lstm_permutation_importance(model, val_loader, dataset.feature_columns, dataset.pm_columns, device)\n",
    "  output_path = f\"{output_dir}/{region}.txt\"\n",
    "\n",
    "  with open(output_path, \"w\") as f:\n",
    "    f.write(\"Feature Importances:\\n\")\n",
    "    for feature, importance in sorted(feature_importances.items(), key=lambda x: x[1], reverse=True):\n",
    "        f.write(f\"{feature}: {importance:.4f}\\n\")\n",
    "  print(f\"Feature importances saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "student",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
