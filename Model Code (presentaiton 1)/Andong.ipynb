{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJwQfT5t0txd"
      },
      "source": [
        "데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BQkNZJ5z0tjT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "file_path = 'kma_2022_andong_noNULL_noDuplicated.csv'  # 파일 경로를 지정하세요.\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert 'TM' column to datetime format\n",
        "data['TM'] = pd.to_datetime(data['TM'], format='%Y%m%d%H%M')\n",
        "\n",
        "# Select relevant columns for prediction\n",
        "selected_columns = ['STN','TM', 'WD', 'WS', 'PA', 'PS', 'TA', 'TD', 'HM', 'CA_TOT', 'CA_MID', 'VS', 'TS', 'TE_005', 'TE_01', 'TE_02','TE_03', 'PV', 'PM10']\n",
        "processed_data = data[selected_columns]\n",
        "\n",
        "# Sort by time\n",
        "processed_data = processed_data.sort_values('TM').reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHIeL0540El2"
      },
      "source": [
        "BASE MODEL : ARIMA 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt2mshEn0CcB",
        "outputId": "a2c4deb4-da1c-467c-97ba-e994840f2ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARIMA Validation RMSE: 13.702848237284295, MAE: 10.553956455224085\n",
            "ARIMA Test RMSE: 24.007840304954517, MAE: 20.149693776969283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
            "  return get_prediction_index(\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
            "  return get_prediction_index(\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
            "  return get_prediction_index(\n"
          ]
        }
      ],
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "arima_data = processed_data[['TM', 'PM10']].set_index('TM')\n",
        "\n",
        "train_size = int(len(arima_data) * 0.7)\n",
        "val_size = int(len(arima_data) * 0.15)\n",
        "\n",
        "train_arima = arima_data.iloc[:train_size]['PM10']\n",
        "y_val = arima_data.iloc[train_size:train_size + val_size]['PM10']\n",
        "y_test = arima_data.iloc[train_size + val_size:]['PM10']\n",
        "arima_order = (5, 1, 0)  # (p, d, q) 값 조정 가능\n",
        "arima_model = ARIMA(train_arima, order=arima_order)\n",
        "arima_fit = arima_model.fit()\n",
        "\n",
        "# 예측\n",
        "forecast_val = arima_fit.forecast(steps=len(y_val))\n",
        "forecast_test = arima_fit.forecast(steps=len(y_test))\n",
        "\n",
        "# 평가\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, forecast_val))\n",
        "val_mae = mean_absolute_error(y_val, forecast_val)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, forecast_test))\n",
        "test_mae = mean_absolute_error(y_test, forecast_test)\n",
        "\n",
        "print(f\"ARIMA Validation RMSE: {val_rmse}, MAE: {val_mae}\")\n",
        "print(f\"ARIMA Test RMSE: {test_rmse}, MAE: {test_mae}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ909Xyq0HVK"
      },
      "source": [
        "MLP model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRxHLtqX0EMu",
        "outputId": "266ae70b-ea3a-4adc-9c8d-3e35def633bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 472.8409 - mae: 16.0177 - val_loss: 121.8925 - val_mae: 8.7467\n",
            "Epoch 2/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 155.7781 - mae: 8.6523 - val_loss: 113.3746 - val_mae: 8.4778\n",
            "Epoch 3/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 141.2676 - mae: 8.3283 - val_loss: 118.7684 - val_mae: 8.6949\n",
            "Epoch 4/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 121.9655 - mae: 7.8959 - val_loss: 132.1961 - val_mae: 9.1647\n",
            "Epoch 5/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 111.9840 - mae: 7.6048 - val_loss: 141.5859 - val_mae: 9.4207\n",
            "Epoch 6/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 110.5999 - mae: 7.4389 - val_loss: 156.0567 - val_mae: 9.8958\n",
            "Epoch 7/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 103.2934 - mae: 7.3368 - val_loss: 160.8002 - val_mae: 9.9788\n",
            "Epoch 8/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 98.6247 - mae: 7.1312 - val_loss: 170.0202 - val_mae: 10.2316\n",
            "Epoch 9/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 100.5268 - mae: 7.2299 - val_loss: 167.1993 - val_mae: 10.1771\n",
            "Epoch 10/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 103.0431 - mae: 7.1754 - val_loss: 177.3838 - val_mae: 10.4276\n",
            "Epoch 11/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 89.8675 - mae: 6.8524 - val_loss: 180.0534 - val_mae: 10.4675\n",
            "Epoch 12/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 93.9099 - mae: 6.9621 - val_loss: 184.9913 - val_mae: 10.5706\n",
            "Epoch 13/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 94.6631 - mae: 6.9025 - val_loss: 188.6178 - val_mae: 10.7131\n",
            "Epoch 14/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 91.2340 - mae: 6.7897 - val_loss: 198.5355 - val_mae: 10.9090\n",
            "Epoch 15/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 85.8261 - mae: 6.7511 - val_loss: 195.2455 - val_mae: 10.7642\n",
            "Epoch 16/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 100.7638 - mae: 6.9657 - val_loss: 214.4210 - val_mae: 11.3191\n",
            "Epoch 17/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 86.8773 - mae: 6.8045 - val_loss: 213.5157 - val_mae: 11.1864\n",
            "Epoch 18/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 81.9396 - mae: 6.6474 - val_loss: 222.3553 - val_mae: 11.4636\n",
            "Epoch 19/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 85.9753 - mae: 6.7443 - val_loss: 226.6125 - val_mae: 11.3921\n",
            "Epoch 20/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 83.8314 - mae: 6.6617 - val_loss: 232.8929 - val_mae: 11.5830\n",
            "Epoch 21/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 79.9642 - mae: 6.5378 - val_loss: 243.0956 - val_mae: 11.7894\n",
            "Epoch 22/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 79.5128 - mae: 6.4867 - val_loss: 261.1163 - val_mae: 12.1689\n",
            "Epoch 23/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 85.6984 - mae: 6.5823 - val_loss: 277.0524 - val_mae: 12.4942\n",
            "Epoch 24/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78.1469 - mae: 6.4273 - val_loss: 280.5718 - val_mae: 12.4997\n",
            "Epoch 25/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 70.9960 - mae: 6.2086 - val_loss: 293.5056 - val_mae: 12.6344\n",
            "Epoch 26/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.5956 - mae: 6.3098 - val_loss: 316.2044 - val_mae: 13.0186\n",
            "Epoch 27/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.2722 - mae: 6.4248 - val_loss: 317.8350 - val_mae: 13.0329\n",
            "Epoch 28/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.9787 - mae: 6.2934 - val_loss: 336.3929 - val_mae: 13.4172\n",
            "Epoch 29/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.2833 - mae: 6.2627 - val_loss: 361.1867 - val_mae: 13.9235\n",
            "Epoch 30/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.8327 - mae: 6.3547 - val_loss: 377.1321 - val_mae: 14.0155\n",
            "Epoch 31/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 70.2119 - mae: 6.2403 - val_loss: 389.3505 - val_mae: 13.9778\n",
            "Epoch 32/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 65.3383 - mae: 6.0902 - val_loss: 397.9150 - val_mae: 14.0040\n",
            "Epoch 33/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 70.2849 - mae: 6.3066 - val_loss: 428.6686 - val_mae: 14.6296\n",
            "Epoch 34/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 65.8348 - mae: 6.0918 - val_loss: 425.3083 - val_mae: 14.4243\n",
            "Epoch 35/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 70.2269 - mae: 6.2269 - val_loss: 430.2633 - val_mae: 14.3846\n",
            "Epoch 36/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 68.1974 - mae: 6.1254 - val_loss: 451.3806 - val_mae: 14.8259\n",
            "Epoch 37/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 63.4567 - mae: 6.0148 - val_loss: 458.3655 - val_mae: 14.8132\n",
            "Epoch 38/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 69.4801 - mae: 6.1493 - val_loss: 482.3242 - val_mae: 15.1223\n",
            "Epoch 39/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 60.3651 - mae: 5.8302 - val_loss: 505.8242 - val_mae: 15.5314\n",
            "Epoch 40/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 65.0612 - mae: 6.0188 - val_loss: 514.1190 - val_mae: 15.6597\n",
            "Epoch 41/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 62.6963 - mae: 5.9196 - val_loss: 532.8947 - val_mae: 15.6188\n",
            "Epoch 42/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 63.5417 - mae: 5.9362 - val_loss: 523.2148 - val_mae: 15.4740\n",
            "Epoch 43/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 64.5515 - mae: 5.9757 - val_loss: 537.6995 - val_mae: 15.7087\n",
            "Epoch 44/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 62.4401 - mae: 5.9301 - val_loss: 551.9752 - val_mae: 15.8574\n",
            "Epoch 45/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 60.6933 - mae: 5.8747 - val_loss: 571.8156 - val_mae: 15.9359\n",
            "Epoch 46/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 57.5060 - mae: 5.8010 - val_loss: 593.6110 - val_mae: 16.2828\n",
            "Epoch 47/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 61.5904 - mae: 5.9319 - val_loss: 580.2061 - val_mae: 16.1282\n",
            "Epoch 48/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 59.4511 - mae: 5.8900 - val_loss: 587.3136 - val_mae: 15.9802\n",
            "Epoch 49/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 63.6277 - mae: 5.9926 - val_loss: 575.3173 - val_mae: 15.9687\n",
            "Epoch 50/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 58.7152 - mae: 5.8815 - val_loss: 619.7468 - val_mae: 16.6931\n",
            "MLP Train Loss (RMSE): 7.792423422335875\n",
            "MLP Validation Loss (RMSE): 24.894713598206323, MAE: 16.69314956665039\n",
            "MLP Test Loss (RMSE): 37.048298422675565, MAE: 31.219411849975586\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = processed_data.drop(columns=['TM', 'PM10'])\n",
        "y = processed_data['PM10']\n",
        "\n",
        "# Split into training, validation, and test sets\n",
        "train_size = 0.7\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=train_size, shuffle=False)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# MLP 모델 설계\n",
        "mlp_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "mlp_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# 학습\n",
        "history = mlp_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_data=(X_val_scaled, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 평가\n",
        "train_loss = history.history['loss'][-1]  # 마지막 epoch의 train loss\n",
        "val_loss, val_mae = mlp_model.evaluate(X_val_scaled, y_val, verbose=0)\n",
        "test_loss, test_mae = mlp_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "\n",
        "print(f\"MLP Train Loss (RMSE): {np.sqrt(train_loss)}\")\n",
        "print(f\"MLP Validation Loss (RMSE): {np.sqrt(val_loss)}, MAE: {val_mae}\")\n",
        "print(f\"MLP Test Loss (RMSE): {np.sqrt(test_loss)}, MAE: {test_mae}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkV-tvBg2l2I",
        "outputId": "5037eaf5-2651-4049-e41a-4ff72092f73d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 427.5121 - mae: 15.2061 - val_loss: 133.2693 - val_mae: 9.1257\n",
            "Epoch 2/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 143.2215 - mae: 8.4794 - val_loss: 111.7285 - val_mae: 8.4143\n",
            "Epoch 3/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 133.8654 - mae: 8.2556 - val_loss: 114.4147 - val_mae: 8.4275\n",
            "Epoch 4/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 120.1380 - mae: 7.8382 - val_loss: 115.1718 - val_mae: 8.4566\n",
            "Epoch 5/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 119.1953 - mae: 7.7500 - val_loss: 135.4169 - val_mae: 9.2224\n",
            "Epoch 6/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 114.9957 - mae: 7.5424 - val_loss: 148.2520 - val_mae: 9.6362\n",
            "Epoch 7/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 118.2020 - mae: 7.6586 - val_loss: 163.1243 - val_mae: 10.1011\n",
            "Epoch 8/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 103.2066 - mae: 7.2351 - val_loss: 185.0685 - val_mae: 10.6768\n",
            "Epoch 9/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 96.5637 - mae: 7.1011 - val_loss: 187.5983 - val_mae: 10.7301\n",
            "Epoch 10/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 95.7337 - mae: 7.0653 - val_loss: 198.0084 - val_mae: 10.8698\n",
            "Epoch 11/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 94.4548 - mae: 6.9926 - val_loss: 202.0494 - val_mae: 11.0206\n",
            "Epoch 12/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 86.5986 - mae: 6.8672 - val_loss: 217.6455 - val_mae: 11.3223\n",
            "Epoch 13/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 91.0108 - mae: 6.9162 - val_loss: 229.8766 - val_mae: 11.6303\n",
            "Epoch 14/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 87.2088 - mae: 6.8328 - val_loss: 256.8615 - val_mae: 12.3541\n",
            "Epoch 15/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 84.2420 - mae: 6.7694 - val_loss: 267.8187 - val_mae: 12.4012\n",
            "Epoch 16/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 85.5747 - mae: 6.7471 - val_loss: 275.4536 - val_mae: 12.2853\n",
            "Epoch 17/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 81.3883 - mae: 6.6882 - val_loss: 285.6609 - val_mae: 12.4552\n",
            "Epoch 18/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78.8261 - mae: 6.5235 - val_loss: 311.9416 - val_mae: 12.8575\n",
            "Epoch 19/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 80.0761 - mae: 6.6028 - val_loss: 330.4052 - val_mae: 13.1442\n",
            "Epoch 20/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.3877 - mae: 6.4212 - val_loss: 357.3820 - val_mae: 13.6313\n",
            "Epoch 21/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.2729 - mae: 6.5022 - val_loss: 358.1576 - val_mae: 13.4850\n",
            "Epoch 22/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 74.4963 - mae: 6.3349 - val_loss: 386.1623 - val_mae: 13.9303\n",
            "Epoch 23/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 70.9665 - mae: 6.3238 - val_loss: 410.9206 - val_mae: 14.2168\n",
            "Epoch 24/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 79.0501 - mae: 6.5459 - val_loss: 414.8328 - val_mae: 14.2752\n",
            "Epoch 25/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 71.7789 - mae: 6.2962 - val_loss: 446.6876 - val_mae: 14.6543\n",
            "Epoch 26/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 76.5122 - mae: 6.3614 - val_loss: 474.3027 - val_mae: 15.0390\n",
            "Epoch 27/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.4432 - mae: 6.4176 - val_loss: 477.0171 - val_mae: 15.0110\n",
            "Epoch 28/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 69.3008 - mae: 6.1949 - val_loss: 480.9880 - val_mae: 14.8012\n",
            "Epoch 29/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 68.6441 - mae: 6.1772 - val_loss: 495.7389 - val_mae: 15.3076\n",
            "Epoch 30/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 64.7342 - mae: 6.0604 - val_loss: 495.5390 - val_mae: 15.1537\n",
            "Epoch 31/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 65.4369 - mae: 6.0879 - val_loss: 545.3383 - val_mae: 15.6328\n",
            "Epoch 32/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 65.4487 - mae: 6.0620 - val_loss: 557.6382 - val_mae: 15.8102\n",
            "Epoch 33/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 69.7647 - mae: 6.2282 - val_loss: 559.3832 - val_mae: 15.6317\n",
            "Epoch 34/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 69.2394 - mae: 6.2439 - val_loss: 583.5161 - val_mae: 16.1640\n",
            "Epoch 35/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 67.8046 - mae: 6.1701 - val_loss: 595.1137 - val_mae: 16.2736\n",
            "Epoch 36/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 63.8718 - mae: 6.0288 - val_loss: 621.8998 - val_mae: 16.3455\n",
            "Epoch 37/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 63.8981 - mae: 6.0324 - val_loss: 635.5087 - val_mae: 16.3637\n",
            "Epoch 38/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 61.6376 - mae: 5.9224 - val_loss: 676.1599 - val_mae: 16.8257\n",
            "Epoch 39/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 59.1672 - mae: 5.8648 - val_loss: 626.3367 - val_mae: 16.6709\n",
            "Epoch 40/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 61.3234 - mae: 6.0290 - val_loss: 693.6425 - val_mae: 17.0566\n",
            "Epoch 41/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 62.1984 - mae: 6.0016 - val_loss: 681.3640 - val_mae: 17.0147\n",
            "Epoch 42/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 61.9195 - mae: 5.9812 - val_loss: 669.2152 - val_mae: 16.5713\n",
            "Epoch 43/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 58.1294 - mae: 5.7716 - val_loss: 674.3677 - val_mae: 16.6852\n",
            "Epoch 44/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 58.4657 - mae: 5.8676 - val_loss: 738.9724 - val_mae: 17.5930\n",
            "Epoch 45/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 57.1682 - mae: 5.7955 - val_loss: 733.2789 - val_mae: 17.1654\n",
            "Epoch 46/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 60.4948 - mae: 5.8474 - val_loss: 811.7109 - val_mae: 18.1044\n",
            "Epoch 47/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 57.2097 - mae: 5.8241 - val_loss: 789.5497 - val_mae: 17.7803\n",
            "Epoch 48/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 61.0959 - mae: 5.9057 - val_loss: 815.6300 - val_mae: 18.1366\n",
            "Epoch 49/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 56.2119 - mae: 5.7095 - val_loss: 818.6404 - val_mae: 18.0428\n",
            "Epoch 50/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 55.7082 - mae: 5.7830 - val_loss: 882.3314 - val_mae: 18.6852\n",
            "Epoch 51/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 56.5232 - mae: 5.7321 - val_loss: 882.4754 - val_mae: 18.4836\n",
            "Epoch 52/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 55.7336 - mae: 5.7165 - val_loss: 878.3390 - val_mae: 18.3076\n",
            "Epoch 53/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 58.6955 - mae: 5.8421 - val_loss: 907.3031 - val_mae: 18.5136\n",
            "Epoch 54/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 53.9194 - mae: 5.6339 - val_loss: 911.3839 - val_mae: 18.6428\n",
            "Epoch 55/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 54.2220 - mae: 5.7209 - val_loss: 917.4508 - val_mae: 18.7250\n",
            "Epoch 56/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 55.7043 - mae: 5.7823 - val_loss: 881.7291 - val_mae: 18.5185\n",
            "Epoch 57/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52.3556 - mae: 5.5482 - val_loss: 940.9225 - val_mae: 18.8399\n",
            "Epoch 58/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 53.8061 - mae: 5.5730 - val_loss: 882.4097 - val_mae: 18.7494\n",
            "Epoch 59/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 52.8210 - mae: 5.6321 - val_loss: 899.5353 - val_mae: 18.6713\n",
            "Epoch 60/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 51.6785 - mae: 5.5536 - val_loss: 875.8646 - val_mae: 18.4325\n",
            "Epoch 61/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52.6045 - mae: 5.5487 - val_loss: 946.0072 - val_mae: 18.8221\n",
            "Epoch 62/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 52.1158 - mae: 5.5519 - val_loss: 937.6370 - val_mae: 19.0198\n",
            "Epoch 63/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52.3893 - mae: 5.5518 - val_loss: 940.1605 - val_mae: 18.7772\n",
            "Epoch 64/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 53.3481 - mae: 5.5599 - val_loss: 842.6445 - val_mae: 18.2144\n",
            "Epoch 65/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 51.2812 - mae: 5.5531 - val_loss: 976.1624 - val_mae: 19.2792\n",
            "Epoch 66/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 54.8393 - mae: 5.5059 - val_loss: 938.7925 - val_mae: 19.2040\n",
            "Epoch 67/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52.6021 - mae: 5.5002 - val_loss: 917.0122 - val_mae: 18.6115\n",
            "Epoch 68/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 51.6268 - mae: 5.4549 - val_loss: 975.0457 - val_mae: 19.3624\n",
            "Epoch 69/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 49.6948 - mae: 5.4767 - val_loss: 929.1689 - val_mae: 19.0005\n",
            "Epoch 70/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 50.9953 - mae: 5.4517 - val_loss: 921.5245 - val_mae: 18.6870\n",
            "Epoch 71/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 50.9690 - mae: 5.5059 - val_loss: 980.8422 - val_mae: 19.3594\n",
            "Epoch 72/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 47.8620 - mae: 5.3685 - val_loss: 940.9271 - val_mae: 18.8575\n",
            "Epoch 73/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 49.0333 - mae: 5.4411 - val_loss: 1018.7633 - val_mae: 19.4501\n",
            "Epoch 74/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 47.5273 - mae: 5.3543 - val_loss: 1136.3773 - val_mae: 20.1805\n",
            "Epoch 75/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 48.8809 - mae: 5.3826 - val_loss: 973.5345 - val_mae: 19.0030\n",
            "Epoch 76/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 48.5218 - mae: 5.3599 - val_loss: 992.1570 - val_mae: 19.1864\n",
            "Epoch 77/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 50.1192 - mae: 5.3749 - val_loss: 1097.3209 - val_mae: 19.8999\n",
            "Epoch 78/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 47.0998 - mae: 5.3058 - val_loss: 1012.5758 - val_mae: 19.2434\n",
            "Epoch 79/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 48.0826 - mae: 5.3315 - val_loss: 994.8087 - val_mae: 19.1933\n",
            "Epoch 80/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 45.0330 - mae: 5.1998 - val_loss: 996.2175 - val_mae: 19.3781\n",
            "Epoch 81/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 49.0474 - mae: 5.3679 - val_loss: 1028.2006 - val_mae: 19.5813\n",
            "Epoch 82/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 46.0318 - mae: 5.2308 - val_loss: 1105.9104 - val_mae: 20.0846\n",
            "Epoch 83/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 45.8378 - mae: 5.2629 - val_loss: 1035.5765 - val_mae: 19.7814\n",
            "Epoch 84/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 48.4148 - mae: 5.3845 - val_loss: 1001.5048 - val_mae: 19.2456\n",
            "Epoch 85/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 47.5377 - mae: 5.3264 - val_loss: 967.2385 - val_mae: 19.1899\n",
            "Epoch 86/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 45.9208 - mae: 5.2376 - val_loss: 899.2191 - val_mae: 18.5853\n",
            "Epoch 87/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 45.4496 - mae: 5.1883 - val_loss: 966.1703 - val_mae: 18.7909\n",
            "Epoch 88/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 45.6513 - mae: 5.2180 - val_loss: 1004.6708 - val_mae: 19.2960\n",
            "Epoch 89/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 46.8056 - mae: 5.2822 - val_loss: 1050.8323 - val_mae: 19.5902\n",
            "Epoch 90/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 42.3365 - mae: 5.0357 - val_loss: 969.6859 - val_mae: 18.9498\n",
            "Epoch 91/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 45.9243 - mae: 5.2262 - val_loss: 949.7036 - val_mae: 19.0826\n",
            "Epoch 92/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 43.8680 - mae: 5.1984 - val_loss: 834.0858 - val_mae: 17.9169\n",
            "Epoch 93/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 44.2348 - mae: 5.1057 - val_loss: 884.2323 - val_mae: 18.5264\n",
            "Epoch 94/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 43.2128 - mae: 5.0912 - val_loss: 988.7960 - val_mae: 19.1468\n",
            "Epoch 95/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 44.5187 - mae: 5.1858 - val_loss: 927.2894 - val_mae: 18.6940\n",
            "Epoch 96/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 44.8907 - mae: 5.1896 - val_loss: 836.9924 - val_mae: 17.9944\n",
            "Epoch 97/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 42.2912 - mae: 5.0390 - val_loss: 910.7824 - val_mae: 18.6026\n",
            "Epoch 98/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 44.2824 - mae: 5.1593 - val_loss: 843.6639 - val_mae: 18.1131\n",
            "Epoch 99/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 40.7688 - mae: 4.9872 - val_loss: 823.8721 - val_mae: 17.8099\n",
            "Epoch 100/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 42.0947 - mae: 5.0446 - val_loss: 816.0696 - val_mae: 17.7730\n",
            "Epoch 101/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 44.2149 - mae: 5.1556 - val_loss: 831.3822 - val_mae: 17.9196\n",
            "Epoch 102/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 43.3513 - mae: 4.9645 - val_loss: 840.6300 - val_mae: 18.3593\n",
            "Epoch 103/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 42.9451 - mae: 5.0650 - val_loss: 720.4821 - val_mae: 17.0260\n",
            "Epoch 104/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 40.6653 - mae: 4.9184 - val_loss: 797.3066 - val_mae: 17.7675\n",
            "Epoch 105/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 42.0077 - mae: 5.0524 - val_loss: 848.3477 - val_mae: 18.0290\n",
            "Epoch 106/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41.6692 - mae: 5.0124 - val_loss: 799.7435 - val_mae: 17.5157\n",
            "Epoch 107/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41.1954 - mae: 5.0321 - val_loss: 703.3631 - val_mae: 16.9385\n",
            "Epoch 108/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 40.6702 - mae: 5.0006 - val_loss: 747.0380 - val_mae: 17.3766\n",
            "Epoch 109/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 42.5242 - mae: 5.0693 - val_loss: 808.7246 - val_mae: 17.9515\n",
            "Epoch 110/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41.3019 - mae: 4.9384 - val_loss: 786.9127 - val_mae: 17.4792\n",
            "Epoch 111/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 40.6803 - mae: 4.9723 - val_loss: 807.3829 - val_mae: 17.7203\n",
            "Epoch 112/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41.2935 - mae: 4.9876 - val_loss: 689.8615 - val_mae: 16.8636\n",
            "Epoch 113/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 41.6936 - mae: 5.0231 - val_loss: 569.3196 - val_mae: 15.5840\n",
            "Epoch 114/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 40.1672 - mae: 4.9212 - val_loss: 673.5588 - val_mae: 16.5845\n",
            "Epoch 115/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39.0183 - mae: 4.8500 - val_loss: 626.2045 - val_mae: 16.2595\n",
            "Epoch 116/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 40.2644 - mae: 4.8970 - val_loss: 657.1718 - val_mae: 16.5783\n",
            "Epoch 117/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 39.8387 - mae: 4.9224 - val_loss: 611.6166 - val_mae: 16.1551\n",
            "Epoch 118/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41.2781 - mae: 4.9738 - val_loss: 600.9851 - val_mae: 15.9170\n",
            "Epoch 119/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 40.4143 - mae: 4.9371 - val_loss: 568.8976 - val_mae: 15.5781\n",
            "Epoch 120/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 38.6692 - mae: 4.8326 - val_loss: 605.9460 - val_mae: 16.4014\n",
            "Epoch 121/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 38.6877 - mae: 4.8315 - val_loss: 615.7846 - val_mae: 16.1426\n",
            "Epoch 122/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 38.1518 - mae: 4.8264 - val_loss: 613.9837 - val_mae: 16.2128\n",
            "Epoch 123/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38.6411 - mae: 4.8096 - val_loss: 554.1129 - val_mae: 15.4624\n",
            "Epoch 124/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37.9154 - mae: 4.8185 - val_loss: 625.2958 - val_mae: 16.2959\n",
            "Epoch 125/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 38.1050 - mae: 4.8093 - val_loss: 585.5914 - val_mae: 15.8722\n",
            "Epoch 126/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 38.9719 - mae: 4.8044 - val_loss: 542.2579 - val_mae: 15.4084\n",
            "Epoch 127/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38.6593 - mae: 4.7427 - val_loss: 628.0610 - val_mae: 16.1844\n",
            "Epoch 128/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37.4038 - mae: 4.7620 - val_loss: 668.0118 - val_mae: 16.4446\n",
            "Epoch 129/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38.9683 - mae: 4.8516 - val_loss: 553.0547 - val_mae: 15.4827\n",
            "Epoch 130/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 38.6945 - mae: 4.7651 - val_loss: 564.9080 - val_mae: 15.6370\n",
            "Epoch 131/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 37.7780 - mae: 4.8314 - val_loss: 562.4225 - val_mae: 15.6916\n",
            "Epoch 132/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38.1949 - mae: 4.7787 - val_loss: 539.1760 - val_mae: 15.2356\n",
            "Epoch 133/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39.2395 - mae: 4.8214 - val_loss: 569.2645 - val_mae: 15.5823\n",
            "Epoch 134/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37.0401 - mae: 4.7433 - val_loss: 580.8185 - val_mae: 15.8398\n",
            "Epoch 135/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38.3427 - mae: 4.8230 - val_loss: 644.7897 - val_mae: 16.4301\n",
            "Epoch 136/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39.2536 - mae: 4.8859 - val_loss: 574.8268 - val_mae: 15.6909\n",
            "Epoch 137/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 35.6077 - mae: 4.6292 - val_loss: 463.6978 - val_mae: 14.5049\n",
            "Epoch 138/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 37.2792 - mae: 4.7209 - val_loss: 539.3185 - val_mae: 15.1539\n",
            "Epoch 139/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36.1055 - mae: 4.6219 - val_loss: 521.3323 - val_mae: 15.1271\n",
            "Epoch 140/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 40.0706 - mae: 4.8966 - val_loss: 551.3321 - val_mae: 15.2815\n",
            "Epoch 141/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 34.7009 - mae: 4.5708 - val_loss: 484.9279 - val_mae: 14.7657\n",
            "Epoch 142/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 38.2253 - mae: 4.7737 - val_loss: 515.1718 - val_mae: 14.8854\n",
            "Epoch 143/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 36.9953 - mae: 4.7307 - val_loss: 581.3239 - val_mae: 15.6688\n",
            "Epoch 144/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 36.4439 - mae: 4.7286 - val_loss: 498.9435 - val_mae: 15.0520\n",
            "Epoch 145/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 36.0892 - mae: 4.6919 - val_loss: 485.7505 - val_mae: 14.8135\n",
            "Epoch 146/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 34.6353 - mae: 4.5579 - val_loss: 574.7695 - val_mae: 15.6185\n",
            "Epoch 147/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 34.3928 - mae: 4.5726 - val_loss: 517.9716 - val_mae: 15.1267\n",
            "Epoch 148/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35.8606 - mae: 4.6485 - val_loss: 448.3477 - val_mae: 14.2609\n",
            "Epoch 149/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35.9096 - mae: 4.6393 - val_loss: 490.7563 - val_mae: 14.6213\n",
            "Epoch 150/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 35.4582 - mae: 4.6122 - val_loss: 456.3525 - val_mae: 14.3664\n",
            "Epoch 151/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 33.5365 - mae: 4.4627 - val_loss: 524.4840 - val_mae: 14.9392\n",
            "Epoch 152/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35.9292 - mae: 4.6819 - val_loss: 504.8087 - val_mae: 14.6451\n",
            "Epoch 153/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 35.6933 - mae: 4.5787 - val_loss: 484.1491 - val_mae: 14.5683\n",
            "Epoch 154/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 32.8511 - mae: 4.4144 - val_loss: 472.7109 - val_mae: 14.7128\n",
            "Epoch 155/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35.8900 - mae: 4.6567 - val_loss: 530.1385 - val_mae: 15.1687\n",
            "Epoch 156/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 34.7991 - mae: 4.5796 - val_loss: 523.9043 - val_mae: 14.9456\n",
            "Epoch 157/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 33.9170 - mae: 4.5115 - val_loss: 508.4947 - val_mae: 14.8952\n",
            "Epoch 158/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 34.6397 - mae: 4.5986 - val_loss: 556.1000 - val_mae: 15.3971\n",
            "Epoch 159/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36.2182 - mae: 4.6419 - val_loss: 497.9437 - val_mae: 14.9511\n",
            "Epoch 160/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 33.8490 - mae: 4.4617 - val_loss: 531.3768 - val_mae: 15.1185\n",
            "Epoch 161/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 34.4563 - mae: 4.5533 - val_loss: 462.6231 - val_mae: 14.4752\n",
            "Epoch 162/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 35.3417 - mae: 4.5725 - val_loss: 559.2955 - val_mae: 15.5536\n",
            "Epoch 163/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 33.9416 - mae: 4.4923 - val_loss: 502.6563 - val_mae: 14.9721\n",
            "Epoch 164/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 35.0000 - mae: 4.5766 - val_loss: 563.7759 - val_mae: 15.4992\n",
            "Epoch 165/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 31.6929 - mae: 4.3647 - val_loss: 533.8878 - val_mae: 15.3329\n",
            "Epoch 166/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31.4816 - mae: 4.3719 - val_loss: 526.9767 - val_mae: 15.0802\n",
            "Epoch 167/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31.9562 - mae: 4.4005 - val_loss: 518.3101 - val_mae: 15.1367\n",
            "Epoch 168/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34.6907 - mae: 4.5198 - val_loss: 472.3671 - val_mae: 14.6556\n",
            "Epoch 169/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34.2801 - mae: 4.5439 - val_loss: 572.8101 - val_mae: 15.5344\n",
            "Epoch 170/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 34.9116 - mae: 4.5752 - val_loss: 511.3735 - val_mae: 15.1067\n",
            "Epoch 171/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 33.4389 - mae: 4.4630 - val_loss: 530.9233 - val_mae: 15.2398\n",
            "Epoch 172/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30.6335 - mae: 4.2730 - val_loss: 531.2134 - val_mae: 15.2578\n",
            "Epoch 173/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 33.4137 - mae: 4.4303 - val_loss: 514.1542 - val_mae: 15.2937\n",
            "Epoch 174/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32.1700 - mae: 4.3662 - val_loss: 563.8680 - val_mae: 15.5172\n",
            "Epoch 175/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 31.5303 - mae: 4.3634 - val_loss: 518.7838 - val_mae: 15.2616\n",
            "Epoch 176/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34.0625 - mae: 4.4854 - val_loss: 505.4819 - val_mae: 15.2412\n",
            "Epoch 177/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 32.9092 - mae: 4.4482 - val_loss: 576.3636 - val_mae: 15.8415\n",
            "Epoch 178/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34.2108 - mae: 4.4931 - val_loss: 635.8303 - val_mae: 16.1789\n",
            "Epoch 179/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31.2780 - mae: 4.3565 - val_loss: 610.2880 - val_mae: 16.1688\n",
            "Epoch 180/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31.4229 - mae: 4.3650 - val_loss: 672.1962 - val_mae: 16.3122\n",
            "Epoch 181/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30.8282 - mae: 4.3087 - val_loss: 579.0105 - val_mae: 15.6686\n",
            "Epoch 182/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31.7253 - mae: 4.3568 - val_loss: 674.6632 - val_mae: 16.8775\n",
            "Epoch 183/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 33.2807 - mae: 4.4574 - val_loss: 713.1945 - val_mae: 17.0703\n",
            "Epoch 184/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 32.4362 - mae: 4.3927 - val_loss: 660.5411 - val_mae: 16.6711\n",
            "Epoch 185/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 33.1159 - mae: 4.4642 - val_loss: 689.5057 - val_mae: 17.0239\n",
            "Epoch 186/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 31.4914 - mae: 4.3617 - val_loss: 647.6577 - val_mae: 16.4415\n",
            "Epoch 187/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 30.4349 - mae: 4.2977 - val_loss: 611.2326 - val_mae: 16.1348\n",
            "Epoch 188/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 32.8370 - mae: 4.3941 - val_loss: 612.4899 - val_mae: 15.8701\n",
            "Epoch 189/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31.2573 - mae: 4.3406 - val_loss: 631.6537 - val_mae: 16.3904\n",
            "Epoch 190/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31.3926 - mae: 4.3013 - val_loss: 629.4541 - val_mae: 16.3145\n",
            "Epoch 191/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 28.9937 - mae: 4.1977 - val_loss: 663.5660 - val_mae: 16.4884\n",
            "Epoch 192/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31.2236 - mae: 4.2923 - val_loss: 637.2853 - val_mae: 16.4699\n",
            "Epoch 193/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 30.8822 - mae: 4.3062 - val_loss: 751.7619 - val_mae: 17.2256\n",
            "Epoch 194/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30.7459 - mae: 4.2758 - val_loss: 658.3090 - val_mae: 16.5166\n",
            "Epoch 195/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 32.8567 - mae: 4.3705 - val_loss: 731.4064 - val_mae: 17.2877\n",
            "Epoch 196/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29.6866 - mae: 4.2434 - val_loss: 711.7334 - val_mae: 17.0466\n",
            "Epoch 197/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30.0008 - mae: 4.2582 - val_loss: 725.2311 - val_mae: 16.8857\n",
            "Epoch 198/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31.5074 - mae: 4.3271 - val_loss: 759.1339 - val_mae: 17.0243\n",
            "Epoch 199/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30.8402 - mae: 4.2683 - val_loss: 721.4644 - val_mae: 16.8173\n",
            "Epoch 200/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 29.9486 - mae: 4.1826 - val_loss: 674.8535 - val_mae: 16.5900\n",
            "MLP Train Loss (RMSE): 5.4919664493099996\n",
            "MLP Validation Loss (RMSE): 25.977942867459696, MAE: 16.58999252319336\n",
            "MLP Test Loss (RMSE): 47.79064553635209, MAE: 38.25637435913086\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = processed_data.drop(columns=['TM', 'PM10'])\n",
        "y = processed_data['PM10']\n",
        "\n",
        "# Split into training, validation, and test sets\n",
        "train_size = 0.7\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=train_size, shuffle=False)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# MLP 모델 설계\n",
        "mlp_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "mlp_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# 학습\n",
        "history = mlp_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_data=(X_val_scaled, y_val),\n",
        "    epochs=200,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 평가\n",
        "train_loss = history.history['loss'][-1]  # 마지막 epoch의 train loss\n",
        "val_loss, val_mae = mlp_model.evaluate(X_val_scaled, y_val, verbose=0)\n",
        "test_loss, test_mae = mlp_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "\n",
        "print(f\"MLP Train Loss (RMSE): {np.sqrt(train_loss)}\")\n",
        "print(f\"MLP Validation Loss (RMSE): {np.sqrt(val_loss)}, MAE: {val_mae}\")\n",
        "print(f\"MLP Test Loss (RMSE): {np.sqrt(test_loss)}, MAE: {test_mae}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}