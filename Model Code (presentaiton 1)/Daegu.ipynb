{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJwQfT5t0txd"
      },
      "source": [
        "데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BQkNZJ5z0tjT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "file_path = 'kma_2022_daegu_noNULL_noDuplicated.csv'  # 파일 경로를 지정하세요.\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert 'TM' column to datetime format\n",
        "data['TM'] = pd.to_datetime(data['TM'], format='%Y%m%d%H%M')\n",
        "\n",
        "# Select relevant columns for prediction\n",
        "selected_columns = ['STN','TM', 'WD', 'WS', 'PA', 'PS', 'TA', 'TD', 'HM', 'CA_TOT', 'CA_MID', 'VS', 'TS', 'TE_005', 'TE_01', 'TE_02','TE_03', 'PV', 'PM10']\n",
        "processed_data = data[selected_columns]\n",
        "\n",
        "# Sort by time\n",
        "processed_data = processed_data.sort_values('TM').reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHIeL0540El2"
      },
      "source": [
        "BASE MODEL : ARIMA 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt2mshEn0CcB",
        "outputId": "742af2f8-2be3-488e-c499-df5dbbee5768"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARIMA Validation RMSE: 12.721291974512154, MAE: 8.901304002376667\n",
            "ARIMA Test RMSE: 28.93017961959995, MAE: 21.288347289218326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
            "  return get_prediction_index(\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
            "  return get_prediction_index(\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
            "  return get_prediction_index(\n"
          ]
        }
      ],
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "arima_data = processed_data[['TM', 'PM10']].set_index('TM')\n",
        "\n",
        "train_size = int(len(arima_data) * 0.7)\n",
        "val_size = int(len(arima_data) * 0.15)\n",
        "\n",
        "train_arima = arima_data.iloc[:train_size]['PM10']\n",
        "y_val = arima_data.iloc[train_size:train_size + val_size]['PM10']\n",
        "y_test = arima_data.iloc[train_size + val_size:]['PM10']\n",
        "arima_order = (5, 1, 0)  # (p, d, q) 값 조정 가능\n",
        "arima_model = ARIMA(train_arima, order=arima_order)\n",
        "arima_fit = arima_model.fit()\n",
        "\n",
        "# 예측\n",
        "forecast_val = arima_fit.forecast(steps=len(y_val))\n",
        "forecast_test = arima_fit.forecast(steps=len(y_test))\n",
        "\n",
        "# 평가\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, forecast_val))\n",
        "val_mae = mean_absolute_error(y_val, forecast_val)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, forecast_test))\n",
        "test_mae = mean_absolute_error(y_test, forecast_test)\n",
        "\n",
        "print(f\"ARIMA Validation RMSE: {val_rmse}, MAE: {val_mae}\")\n",
        "print(f\"ARIMA Test RMSE: {test_rmse}, MAE: {test_mae}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ909Xyq0HVK"
      },
      "source": [
        "MLP model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRxHLtqX0EMu",
        "outputId": "9a6e57fe-243f-45eb-d3e6-fe9bd1d48728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 568.6674 - mae: 16.8738 - val_loss: 135.1087 - val_mae: 9.2804\n",
            "Epoch 2/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 214.2451 - mae: 9.9818 - val_loss: 105.7214 - val_mae: 8.0522\n",
            "Epoch 3/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 179.8572 - mae: 9.2724 - val_loss: 111.5070 - val_mae: 8.3125\n",
            "Epoch 4/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 167.1351 - mae: 9.1878 - val_loss: 114.4706 - val_mae: 8.3208\n",
            "Epoch 5/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 172.2024 - mae: 9.1485 - val_loss: 115.6646 - val_mae: 8.4426\n",
            "Epoch 6/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 159.1839 - mae: 8.8730 - val_loss: 105.4120 - val_mae: 7.9472\n",
            "Epoch 7/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 170.3184 - mae: 9.1659 - val_loss: 99.5498 - val_mae: 7.6907\n",
            "Epoch 8/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 152.8664 - mae: 8.7021 - val_loss: 97.9596 - val_mae: 7.6132\n",
            "Epoch 9/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 144.4418 - mae: 8.4895 - val_loss: 97.4972 - val_mae: 7.5035\n",
            "Epoch 10/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 187.8547 - mae: 8.6180 - val_loss: 100.6853 - val_mae: 7.6528\n",
            "Epoch 11/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 137.7519 - mae: 8.3256 - val_loss: 109.2322 - val_mae: 8.0062\n",
            "Epoch 12/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 142.7982 - mae: 8.3953 - val_loss: 94.6017 - val_mae: 7.4773\n",
            "Epoch 13/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 142.0009 - mae: 8.2312 - val_loss: 100.2306 - val_mae: 7.6981\n",
            "Epoch 14/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 129.8671 - mae: 8.1486 - val_loss: 105.9412 - val_mae: 7.8051\n",
            "Epoch 15/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 122.5898 - mae: 8.0646 - val_loss: 107.6993 - val_mae: 7.9784\n",
            "Epoch 16/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 160.1030 - mae: 8.2733 - val_loss: 101.8970 - val_mae: 7.7142\n",
            "Epoch 17/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 137.1160 - mae: 8.1315 - val_loss: 108.6227 - val_mae: 7.9498\n",
            "Epoch 18/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 130.7282 - mae: 8.0627 - val_loss: 107.3593 - val_mae: 7.8280\n",
            "Epoch 19/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 138.7783 - mae: 8.1418 - val_loss: 100.3220 - val_mae: 7.6568\n",
            "Epoch 20/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 130.6107 - mae: 7.9724 - val_loss: 105.2296 - val_mae: 7.7901\n",
            "Epoch 21/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 137.8387 - mae: 8.0505 - val_loss: 101.0275 - val_mae: 7.6336\n",
            "Epoch 22/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 133.9483 - mae: 7.8878 - val_loss: 103.2945 - val_mae: 7.7083\n",
            "Epoch 23/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 124.6792 - mae: 7.9170 - val_loss: 112.3324 - val_mae: 8.0242\n",
            "Epoch 24/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 121.9165 - mae: 7.7257 - val_loss: 114.5969 - val_mae: 8.0137\n",
            "Epoch 25/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 117.9798 - mae: 7.6159 - val_loss: 106.2596 - val_mae: 7.8035\n",
            "Epoch 26/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 118.8213 - mae: 7.6863 - val_loss: 110.6462 - val_mae: 7.9903\n",
            "Epoch 27/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 127.0757 - mae: 7.6688 - val_loss: 112.5921 - val_mae: 7.9586\n",
            "Epoch 28/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 119.9828 - mae: 7.5736 - val_loss: 111.0796 - val_mae: 7.9832\n",
            "Epoch 29/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 121.1405 - mae: 7.6344 - val_loss: 105.9464 - val_mae: 7.8276\n",
            "Epoch 30/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 105.0920 - mae: 7.4719 - val_loss: 123.3484 - val_mae: 8.3174\n",
            "Epoch 31/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 139.8520 - mae: 7.8146 - val_loss: 111.9472 - val_mae: 7.9946\n",
            "Epoch 32/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 113.7273 - mae: 7.5636 - val_loss: 117.0573 - val_mae: 8.3195\n",
            "Epoch 33/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 101.1757 - mae: 7.4561 - val_loss: 120.5200 - val_mae: 8.3094\n",
            "Epoch 34/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 103.2854 - mae: 7.6196 - val_loss: 113.2392 - val_mae: 7.9931\n",
            "Epoch 35/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 111.7098 - mae: 7.3925 - val_loss: 114.1947 - val_mae: 8.0611\n",
            "Epoch 36/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 122.3983 - mae: 7.5352 - val_loss: 115.4385 - val_mae: 8.1634\n",
            "Epoch 37/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 120.8503 - mae: 7.3418 - val_loss: 119.0315 - val_mae: 8.2148\n",
            "Epoch 38/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 98.5345 - mae: 7.2809 - val_loss: 114.9562 - val_mae: 8.1207\n",
            "Epoch 39/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 95.0611 - mae: 7.1466 - val_loss: 120.9316 - val_mae: 8.3681\n",
            "Epoch 40/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 116.8871 - mae: 7.4973 - val_loss: 125.1973 - val_mae: 8.4045\n",
            "Epoch 41/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 101.3199 - mae: 7.3176 - val_loss: 148.1368 - val_mae: 9.0829\n",
            "Epoch 42/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 98.0667 - mae: 7.2902 - val_loss: 117.1102 - val_mae: 8.0952\n",
            "Epoch 43/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 101.1847 - mae: 7.2718 - val_loss: 112.4972 - val_mae: 8.0225\n",
            "Epoch 44/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 105.9258 - mae: 7.2133 - val_loss: 123.1040 - val_mae: 8.3159\n",
            "Epoch 45/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 94.9009 - mae: 7.1219 - val_loss: 120.2395 - val_mae: 8.2385\n",
            "Epoch 46/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 102.0546 - mae: 7.2710 - val_loss: 138.8642 - val_mae: 8.8064\n",
            "Epoch 47/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 101.9903 - mae: 7.2603 - val_loss: 129.0002 - val_mae: 8.4130\n",
            "Epoch 48/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 98.0636 - mae: 7.2580 - val_loss: 120.4873 - val_mae: 8.2221\n",
            "Epoch 49/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 96.1697 - mae: 7.0705 - val_loss: 123.2758 - val_mae: 8.2421\n",
            "Epoch 50/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 91.9224 - mae: 7.1220 - val_loss: 130.8472 - val_mae: 8.4809\n",
            "MLP Train Loss (RMSE): 10.053111370007468\n",
            "MLP Validation Loss (RMSE): 11.438846696460146, MAE: 8.480871200561523\n",
            "MLP Test Loss (RMSE): 22.185410608047267, MAE: 15.374044418334961\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = processed_data.drop(columns=['TM', 'PM10'])\n",
        "y = processed_data['PM10']\n",
        "\n",
        "# Split into training, validation, and test sets\n",
        "train_size = 0.7\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=train_size, shuffle=False)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# MLP 모델 설계\n",
        "mlp_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "mlp_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# 학습\n",
        "history = mlp_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_data=(X_val_scaled, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 평가\n",
        "train_loss = history.history['loss'][-1]  # 마지막 epoch의 train loss\n",
        "val_loss, val_mae = mlp_model.evaluate(X_val_scaled, y_val, verbose=0)\n",
        "test_loss, test_mae = mlp_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "\n",
        "print(f\"MLP Train Loss (RMSE): {np.sqrt(train_loss)}\")\n",
        "print(f\"MLP Validation Loss (RMSE): {np.sqrt(val_loss)}, MAE: {val_mae}\")\n",
        "print(f\"MLP Test Loss (RMSE): {np.sqrt(test_loss)}, MAE: {test_mae}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkV-tvBg2l2I",
        "outputId": "0f8c7f89-88f6-4dd5-fc67-69b9c53d70ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 561.1057 - mae: 16.8528 - val_loss: 140.1405 - val_mae: 9.4068\n",
            "Epoch 2/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 196.8285 - mae: 9.8833 - val_loss: 117.6642 - val_mae: 8.5686\n",
            "Epoch 3/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 185.2739 - mae: 9.4458 - val_loss: 109.3998 - val_mae: 8.1185\n",
            "Epoch 4/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 187.9179 - mae: 9.3338 - val_loss: 114.4368 - val_mae: 8.3601\n",
            "Epoch 5/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 168.0638 - mae: 8.9981 - val_loss: 105.5048 - val_mae: 8.0848\n",
            "Epoch 6/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 171.2551 - mae: 8.8735 - val_loss: 98.9988 - val_mae: 7.6415\n",
            "Epoch 7/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 171.0592 - mae: 8.8965 - val_loss: 99.1021 - val_mae: 7.6883\n",
            "Epoch 8/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 167.1849 - mae: 8.7139 - val_loss: 110.0245 - val_mae: 8.1479\n",
            "Epoch 9/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 157.5058 - mae: 8.7400 - val_loss: 96.0076 - val_mae: 7.4735\n",
            "Epoch 10/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 141.6102 - mae: 8.3542 - val_loss: 103.1158 - val_mae: 7.7682\n",
            "Epoch 11/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 128.3338 - mae: 8.2375 - val_loss: 105.0866 - val_mae: 7.8573\n",
            "Epoch 12/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 144.4869 - mae: 8.4239 - val_loss: 116.5540 - val_mae: 8.3379\n",
            "Epoch 13/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 126.6864 - mae: 8.1533 - val_loss: 106.4210 - val_mae: 7.9204\n",
            "Epoch 14/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 139.0291 - mae: 8.2553 - val_loss: 117.2461 - val_mae: 8.2654\n",
            "Epoch 15/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 131.3662 - mae: 8.2107 - val_loss: 99.2694 - val_mae: 7.6026\n",
            "Epoch 16/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 145.8407 - mae: 8.3169 - val_loss: 104.3973 - val_mae: 7.8061\n",
            "Epoch 17/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 137.9748 - mae: 8.0850 - val_loss: 100.4731 - val_mae: 7.7248\n",
            "Epoch 18/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 126.9184 - mae: 8.0029 - val_loss: 101.9797 - val_mae: 7.7089\n",
            "Epoch 19/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 134.2898 - mae: 8.0420 - val_loss: 101.8181 - val_mae: 7.7423\n",
            "Epoch 20/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 122.9535 - mae: 7.8010 - val_loss: 116.3301 - val_mae: 8.4358\n",
            "Epoch 21/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 124.6646 - mae: 7.9265 - val_loss: 107.1300 - val_mae: 7.9143\n",
            "Epoch 22/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 118.8790 - mae: 7.7981 - val_loss: 113.1464 - val_mae: 8.1044\n",
            "Epoch 23/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 126.1947 - mae: 7.9664 - val_loss: 116.9214 - val_mae: 8.1753\n",
            "Epoch 24/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 110.8858 - mae: 7.6693 - val_loss: 118.6712 - val_mae: 8.2824\n",
            "Epoch 25/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 134.8675 - mae: 7.9068 - val_loss: 105.0448 - val_mae: 7.7904\n",
            "Epoch 26/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 119.0189 - mae: 7.7728 - val_loss: 116.6191 - val_mae: 8.2254\n",
            "Epoch 27/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 140.6710 - mae: 7.6372 - val_loss: 113.0929 - val_mae: 8.1094\n",
            "Epoch 28/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 113.4052 - mae: 7.5187 - val_loss: 118.4433 - val_mae: 8.2626\n",
            "Epoch 29/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 113.3993 - mae: 7.6047 - val_loss: 122.7149 - val_mae: 8.4086\n",
            "Epoch 30/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 116.3916 - mae: 7.6571 - val_loss: 111.6415 - val_mae: 8.0528\n",
            "Epoch 31/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 121.0554 - mae: 7.7301 - val_loss: 123.7682 - val_mae: 8.4927\n",
            "Epoch 32/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 112.0504 - mae: 7.4367 - val_loss: 113.8358 - val_mae: 8.1052\n",
            "Epoch 33/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 105.3053 - mae: 7.4401 - val_loss: 117.2894 - val_mae: 8.2306\n",
            "Epoch 34/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 116.2852 - mae: 7.4239 - val_loss: 121.6033 - val_mae: 8.3871\n",
            "Epoch 35/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 122.1137 - mae: 7.5995 - val_loss: 118.4298 - val_mae: 8.2501\n",
            "Epoch 36/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 123.3934 - mae: 7.4699 - val_loss: 124.7737 - val_mae: 8.4629\n",
            "Epoch 37/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 109.0446 - mae: 7.3385 - val_loss: 121.5855 - val_mae: 8.3924\n",
            "Epoch 38/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 125.6253 - mae: 7.3586 - val_loss: 127.1612 - val_mae: 8.5091\n",
            "Epoch 39/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 105.0607 - mae: 7.3261 - val_loss: 124.1761 - val_mae: 8.4454\n",
            "Epoch 40/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 124.0781 - mae: 7.3894 - val_loss: 121.8828 - val_mae: 8.3072\n",
            "Epoch 41/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 108.0301 - mae: 7.1718 - val_loss: 127.6063 - val_mae: 8.5314\n",
            "Epoch 42/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 86.5814 - mae: 7.0034 - val_loss: 115.9282 - val_mae: 8.2011\n",
            "Epoch 43/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 89.4923 - mae: 7.0554 - val_loss: 141.3820 - val_mae: 8.9648\n",
            "Epoch 44/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 107.5485 - mae: 7.1988 - val_loss: 137.7293 - val_mae: 8.8846\n",
            "Epoch 45/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 106.5217 - mae: 7.1861 - val_loss: 128.7510 - val_mae: 8.4821\n",
            "Epoch 46/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 97.8933 - mae: 7.1549 - val_loss: 122.2585 - val_mae: 8.4339\n",
            "Epoch 47/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 108.5861 - mae: 7.1714 - val_loss: 126.8834 - val_mae: 8.5221\n",
            "Epoch 48/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 97.6296 - mae: 7.1192 - val_loss: 117.5760 - val_mae: 8.1003\n",
            "Epoch 49/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 92.0068 - mae: 6.9153 - val_loss: 125.8641 - val_mae: 8.4886\n",
            "Epoch 50/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 97.9012 - mae: 7.0342 - val_loss: 124.8765 - val_mae: 8.5254\n",
            "Epoch 51/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 102.9595 - mae: 7.0736 - val_loss: 124.5992 - val_mae: 8.5142\n",
            "Epoch 52/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 97.4855 - mae: 6.8323 - val_loss: 128.1110 - val_mae: 8.5666\n",
            "Epoch 53/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 86.3139 - mae: 6.7895 - val_loss: 125.1730 - val_mae: 8.5043\n",
            "Epoch 54/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 105.2719 - mae: 7.1678 - val_loss: 123.7111 - val_mae: 8.3714\n",
            "Epoch 55/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 90.8696 - mae: 6.8214 - val_loss: 133.9233 - val_mae: 8.6999\n",
            "Epoch 56/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 98.0620 - mae: 7.0154 - val_loss: 147.7626 - val_mae: 9.1657\n",
            "Epoch 57/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 97.9656 - mae: 6.9463 - val_loss: 130.1728 - val_mae: 8.6873\n",
            "Epoch 58/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 106.5836 - mae: 7.0478 - val_loss: 141.3846 - val_mae: 8.9936\n",
            "Epoch 59/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 93.7725 - mae: 6.9658 - val_loss: 124.0459 - val_mae: 8.4345\n",
            "Epoch 60/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 104.1496 - mae: 6.9115 - val_loss: 122.3186 - val_mae: 8.4931\n",
            "Epoch 61/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 90.5007 - mae: 6.7866 - val_loss: 140.1343 - val_mae: 8.9076\n",
            "Epoch 62/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 82.7783 - mae: 6.6915 - val_loss: 122.3038 - val_mae: 8.4715\n",
            "Epoch 63/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 81.0440 - mae: 6.7437 - val_loss: 126.2157 - val_mae: 8.4623\n",
            "Epoch 64/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 82.4766 - mae: 6.7092 - val_loss: 134.5360 - val_mae: 8.8143\n",
            "Epoch 65/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 90.1166 - mae: 6.7872 - val_loss: 128.4325 - val_mae: 8.5073\n",
            "Epoch 66/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 90.9465 - mae: 6.7342 - val_loss: 136.5103 - val_mae: 8.8207\n",
            "Epoch 67/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 82.0409 - mae: 6.7525 - val_loss: 129.9412 - val_mae: 8.6393\n",
            "Epoch 68/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 92.8864 - mae: 6.7375 - val_loss: 131.3481 - val_mae: 8.7119\n",
            "Epoch 69/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 88.6392 - mae: 6.7839 - val_loss: 126.2290 - val_mae: 8.6268\n",
            "Epoch 70/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 95.4358 - mae: 6.8630 - val_loss: 125.4389 - val_mae: 8.4591\n",
            "Epoch 71/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 101.9726 - mae: 6.6519 - val_loss: 125.1646 - val_mae: 8.5015\n",
            "Epoch 72/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 91.3553 - mae: 6.7823 - val_loss: 127.1484 - val_mae: 8.5759\n",
            "Epoch 73/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 87.9558 - mae: 6.6432 - val_loss: 127.5090 - val_mae: 8.5960\n",
            "Epoch 74/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 89.7721 - mae: 6.7955 - val_loss: 134.5076 - val_mae: 8.7279\n",
            "Epoch 75/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 86.6425 - mae: 6.6708 - val_loss: 123.5894 - val_mae: 8.4978\n",
            "Epoch 76/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 87.1771 - mae: 6.6355 - val_loss: 118.5068 - val_mae: 8.2792\n",
            "Epoch 77/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 85.6966 - mae: 6.6153 - val_loss: 140.8355 - val_mae: 8.9330\n",
            "Epoch 78/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 80.1012 - mae: 6.5459 - val_loss: 133.0602 - val_mae: 8.7792\n",
            "Epoch 79/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 83.8418 - mae: 6.5227 - val_loss: 133.6456 - val_mae: 8.7706\n",
            "Epoch 80/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.5772 - mae: 6.5300 - val_loss: 131.2807 - val_mae: 8.6753\n",
            "Epoch 81/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 79.9210 - mae: 6.5593 - val_loss: 129.3712 - val_mae: 8.6714\n",
            "Epoch 82/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 86.3220 - mae: 6.6196 - val_loss: 137.8875 - val_mae: 8.8360\n",
            "Epoch 83/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 90.1691 - mae: 6.7922 - val_loss: 144.4485 - val_mae: 8.9899\n",
            "Epoch 84/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 77.0927 - mae: 6.5798 - val_loss: 139.6892 - val_mae: 8.9082\n",
            "Epoch 85/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 90.5568 - mae: 6.6215 - val_loss: 138.4165 - val_mae: 8.9088\n",
            "Epoch 86/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 89.8872 - mae: 6.5464 - val_loss: 140.8932 - val_mae: 8.9210\n",
            "Epoch 87/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85.2049 - mae: 6.5489 - val_loss: 138.7651 - val_mae: 8.8603\n",
            "Epoch 88/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 77.5736 - mae: 6.4205 - val_loss: 118.2766 - val_mae: 8.2983\n",
            "Epoch 89/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 87.1622 - mae: 6.4894 - val_loss: 129.5696 - val_mae: 8.5761\n",
            "Epoch 90/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 87.5819 - mae: 6.5541 - val_loss: 161.1789 - val_mae: 9.5292\n",
            "Epoch 91/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78.8552 - mae: 6.5669 - val_loss: 136.5342 - val_mae: 8.7147\n",
            "Epoch 92/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 90.8301 - mae: 6.5008 - val_loss: 137.0537 - val_mae: 8.7944\n",
            "Epoch 93/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.3893 - mae: 6.3416 - val_loss: 143.8261 - val_mae: 8.8524\n",
            "Epoch 94/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 98.5779 - mae: 6.6918 - val_loss: 132.9838 - val_mae: 8.7271\n",
            "Epoch 95/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.8861 - mae: 6.5033 - val_loss: 136.2988 - val_mae: 8.7511\n",
            "Epoch 96/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 72.9921 - mae: 6.2487 - val_loss: 143.6309 - val_mae: 9.0827\n",
            "Epoch 97/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85.7254 - mae: 6.4494 - val_loss: 126.5861 - val_mae: 8.5703\n",
            "Epoch 98/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 89.2114 - mae: 6.3507 - val_loss: 139.7574 - val_mae: 8.8314\n",
            "Epoch 99/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 93.5829 - mae: 6.6236 - val_loss: 124.1442 - val_mae: 8.4214\n",
            "Epoch 100/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 85.3112 - mae: 6.3454 - val_loss: 139.2481 - val_mae: 8.7844\n",
            "Epoch 101/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 101.6215 - mae: 6.5812 - val_loss: 131.6003 - val_mae: 8.6378\n",
            "Epoch 102/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 84.8865 - mae: 6.3192 - val_loss: 140.8282 - val_mae: 8.8792\n",
            "Epoch 103/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 79.1975 - mae: 6.2667 - val_loss: 120.8973 - val_mae: 8.4160\n",
            "Epoch 104/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 89.2261 - mae: 6.3985 - val_loss: 131.7718 - val_mae: 8.7134\n",
            "Epoch 105/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 84.4059 - mae: 6.3561 - val_loss: 127.8394 - val_mae: 8.4725\n",
            "Epoch 106/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 88.2902 - mae: 6.3456 - val_loss: 135.3834 - val_mae: 8.6694\n",
            "Epoch 107/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 99.4129 - mae: 6.4173 - val_loss: 132.5591 - val_mae: 8.5869\n",
            "Epoch 108/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 77.8711 - mae: 6.2780 - val_loss: 141.7285 - val_mae: 8.8311\n",
            "Epoch 109/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 67.6894 - mae: 6.1856 - val_loss: 124.9473 - val_mae: 8.5015\n",
            "Epoch 110/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 73.1649 - mae: 6.2586 - val_loss: 132.2605 - val_mae: 8.6892\n",
            "Epoch 111/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 96.7691 - mae: 6.5495 - val_loss: 134.3788 - val_mae: 8.6585\n",
            "Epoch 112/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 68.3430 - mae: 6.1202 - val_loss: 151.4506 - val_mae: 9.2172\n",
            "Epoch 113/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 74.2221 - mae: 6.1883 - val_loss: 143.7833 - val_mae: 9.0170\n",
            "Epoch 114/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 67.4921 - mae: 6.0589 - val_loss: 132.3982 - val_mae: 8.6787\n",
            "Epoch 115/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 102.6714 - mae: 6.5463 - val_loss: 127.3839 - val_mae: 8.5078\n",
            "Epoch 116/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.4787 - mae: 6.1174 - val_loss: 132.4891 - val_mae: 8.6837\n",
            "Epoch 117/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.6109 - mae: 6.0970 - val_loss: 134.6434 - val_mae: 8.6731\n",
            "Epoch 118/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 67.7103 - mae: 6.1085 - val_loss: 136.0004 - val_mae: 8.6881\n",
            "Epoch 119/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 75.7123 - mae: 6.1250 - val_loss: 127.8887 - val_mae: 8.5113\n",
            "Epoch 120/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 64.9141 - mae: 6.0134 - val_loss: 135.2837 - val_mae: 8.7459\n",
            "Epoch 121/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 80.5583 - mae: 6.2068 - val_loss: 130.4650 - val_mae: 8.4732\n",
            "Epoch 122/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.8682 - mae: 6.0694 - val_loss: 128.5399 - val_mae: 8.5871\n",
            "Epoch 123/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.7478 - mae: 5.9714 - val_loss: 133.7720 - val_mae: 8.7349\n",
            "Epoch 124/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 65.5115 - mae: 5.9822 - val_loss: 132.2408 - val_mae: 8.6883\n",
            "Epoch 125/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 91.0840 - mae: 6.3577 - val_loss: 130.3839 - val_mae: 8.5894\n",
            "Epoch 126/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 87.7030 - mae: 6.2007 - val_loss: 137.7453 - val_mae: 8.8143\n",
            "Epoch 127/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 72.4419 - mae: 6.1765 - val_loss: 132.4705 - val_mae: 8.6722\n",
            "Epoch 128/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 69.9552 - mae: 6.1625 - val_loss: 134.8372 - val_mae: 8.7471\n",
            "Epoch 129/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 68.8378 - mae: 6.0742 - val_loss: 139.8931 - val_mae: 8.8679\n",
            "Epoch 130/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 80.5836 - mae: 6.1082 - val_loss: 154.8101 - val_mae: 9.1997\n",
            "Epoch 131/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 89.6642 - mae: 6.0772 - val_loss: 146.0604 - val_mae: 9.0388\n",
            "Epoch 132/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 63.9805 - mae: 6.0117 - val_loss: 130.3988 - val_mae: 8.6206\n",
            "Epoch 133/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 70.1178 - mae: 5.9756 - val_loss: 147.2073 - val_mae: 9.1283\n",
            "Epoch 134/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 68.5680 - mae: 5.8668 - val_loss: 131.5572 - val_mae: 8.5954\n",
            "Epoch 135/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 59.9264 - mae: 5.9021 - val_loss: 139.1462 - val_mae: 8.8196\n",
            "Epoch 136/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 61.5791 - mae: 5.8682 - val_loss: 142.8516 - val_mae: 8.8915\n",
            "Epoch 137/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 63.8619 - mae: 5.9395 - val_loss: 132.0517 - val_mae: 8.6114\n",
            "Epoch 138/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 72.9591 - mae: 5.9554 - val_loss: 140.2338 - val_mae: 8.8825\n",
            "Epoch 139/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 62.7793 - mae: 5.9417 - val_loss: 147.0507 - val_mae: 9.0004\n",
            "Epoch 140/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 77.5803 - mae: 6.1865 - val_loss: 135.9100 - val_mae: 8.7893\n",
            "Epoch 141/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 69.0953 - mae: 5.9783 - val_loss: 142.1086 - val_mae: 9.0995\n",
            "Epoch 142/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.8016 - mae: 5.8800 - val_loss: 146.6986 - val_mae: 9.1856\n",
            "Epoch 143/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 78.0025 - mae: 6.1150 - val_loss: 139.5114 - val_mae: 8.7262\n",
            "Epoch 144/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 69.6054 - mae: 5.9004 - val_loss: 135.6704 - val_mae: 8.7717\n",
            "Epoch 145/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 67.1473 - mae: 5.9420 - val_loss: 131.5359 - val_mae: 8.6155\n",
            "Epoch 146/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 66.8640 - mae: 5.8803 - val_loss: 144.9783 - val_mae: 9.0017\n",
            "Epoch 147/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 71.4580 - mae: 5.9878 - val_loss: 138.3793 - val_mae: 8.9747\n",
            "Epoch 148/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 60.0908 - mae: 5.7415 - val_loss: 184.5256 - val_mae: 10.1007\n",
            "Epoch 149/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.2374 - mae: 6.1406 - val_loss: 136.7863 - val_mae: 8.7610\n",
            "Epoch 150/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 58.6625 - mae: 5.7254 - val_loss: 136.7499 - val_mae: 8.7194\n",
            "Epoch 151/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 66.7830 - mae: 5.8442 - val_loss: 150.3291 - val_mae: 9.1048\n",
            "Epoch 152/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 74.3401 - mae: 5.9191 - val_loss: 133.3428 - val_mae: 8.6576\n",
            "Epoch 153/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 72.6334 - mae: 5.9200 - val_loss: 144.9201 - val_mae: 8.9703\n",
            "Epoch 154/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 67.0455 - mae: 5.9386 - val_loss: 136.9827 - val_mae: 8.7757\n",
            "Epoch 155/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 60.7964 - mae: 5.8273 - val_loss: 151.7773 - val_mae: 9.1809\n",
            "Epoch 156/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 62.9359 - mae: 5.7536 - val_loss: 144.3283 - val_mae: 8.9773\n",
            "Epoch 157/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 61.1989 - mae: 5.5613 - val_loss: 136.9890 - val_mae: 8.8185\n",
            "Epoch 158/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 75.0171 - mae: 5.6946 - val_loss: 158.5905 - val_mae: 9.3985\n",
            "Epoch 159/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.8452 - mae: 5.9027 - val_loss: 141.7997 - val_mae: 8.9749\n",
            "Epoch 160/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 62.4129 - mae: 5.8106 - val_loss: 131.1432 - val_mae: 8.6104\n",
            "Epoch 161/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 69.8216 - mae: 5.8081 - val_loss: 133.9057 - val_mae: 8.6498\n",
            "Epoch 162/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 86.3358 - mae: 5.8702 - val_loss: 150.6901 - val_mae: 9.2055\n",
            "Epoch 163/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 71.1863 - mae: 5.7196 - val_loss: 138.9572 - val_mae: 8.7807\n",
            "Epoch 164/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 89.9051 - mae: 6.0820 - val_loss: 136.5557 - val_mae: 8.8242\n",
            "Epoch 165/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 65.6885 - mae: 5.7703 - val_loss: 144.7114 - val_mae: 9.0745\n",
            "Epoch 166/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 62.3801 - mae: 5.6381 - val_loss: 145.9426 - val_mae: 9.0959\n",
            "Epoch 167/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 57.3428 - mae: 5.6456 - val_loss: 143.8887 - val_mae: 8.9785\n",
            "Epoch 168/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 56.8863 - mae: 5.5428 - val_loss: 150.7263 - val_mae: 9.2223\n",
            "Epoch 169/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69.1795 - mae: 5.6587 - val_loss: 149.6286 - val_mae: 9.1587\n",
            "Epoch 170/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 55.7475 - mae: 5.5472 - val_loss: 139.1570 - val_mae: 8.8848\n",
            "Epoch 171/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 62.4856 - mae: 5.5799 - val_loss: 142.3229 - val_mae: 9.0011\n",
            "Epoch 172/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 55.7686 - mae: 5.5013 - val_loss: 144.2637 - val_mae: 8.9751\n",
            "Epoch 173/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 53.8163 - mae: 5.5838 - val_loss: 142.6983 - val_mae: 8.9240\n",
            "Epoch 174/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 62.7077 - mae: 5.5438 - val_loss: 143.1381 - val_mae: 9.0354\n",
            "Epoch 175/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 60.0041 - mae: 5.6405 - val_loss: 155.5857 - val_mae: 9.2910\n",
            "Epoch 176/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.0258 - mae: 5.8703 - val_loss: 146.7881 - val_mae: 9.0794\n",
            "Epoch 177/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 70.2125 - mae: 5.6982 - val_loss: 147.3227 - val_mae: 9.1526\n",
            "Epoch 178/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 59.4350 - mae: 5.5623 - val_loss: 131.1910 - val_mae: 8.5844\n",
            "Epoch 179/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 69.2814 - mae: 5.6130 - val_loss: 137.4713 - val_mae: 8.8022\n",
            "Epoch 180/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 56.9806 - mae: 5.4679 - val_loss: 150.0248 - val_mae: 9.1035\n",
            "Epoch 181/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 64.2323 - mae: 5.4746 - val_loss: 146.6015 - val_mae: 8.9722\n",
            "Epoch 182/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 64.0333 - mae: 5.6183 - val_loss: 149.6833 - val_mae: 9.1831\n",
            "Epoch 183/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 66.2554 - mae: 5.6220 - val_loss: 140.9763 - val_mae: 8.9392\n",
            "Epoch 184/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 59.3487 - mae: 5.5094 - val_loss: 148.8249 - val_mae: 9.1159\n",
            "Epoch 185/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52.9238 - mae: 5.3260 - val_loss: 150.5624 - val_mae: 9.0795\n",
            "Epoch 186/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 58.8832 - mae: 5.6007 - val_loss: 145.2974 - val_mae: 9.0337\n",
            "Epoch 187/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 61.8552 - mae: 5.5838 - val_loss: 156.8747 - val_mae: 9.2733\n",
            "Epoch 188/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 54.5147 - mae: 5.4945 - val_loss: 153.8972 - val_mae: 9.2535\n",
            "Epoch 189/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 60.1790 - mae: 5.4310 - val_loss: 157.6164 - val_mae: 9.2166\n",
            "Epoch 190/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 69.2142 - mae: 5.5671 - val_loss: 137.8460 - val_mae: 8.7737\n",
            "Epoch 191/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 62.9644 - mae: 5.6569 - val_loss: 150.5392 - val_mae: 9.0839\n",
            "Epoch 192/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 64.4566 - mae: 5.5834 - val_loss: 152.6917 - val_mae: 9.2428\n",
            "Epoch 193/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 63.3604 - mae: 5.4739 - val_loss: 153.1337 - val_mae: 9.1680\n",
            "Epoch 194/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52.3468 - mae: 5.3135 - val_loss: 146.9457 - val_mae: 9.0956\n",
            "Epoch 195/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 59.4305 - mae: 5.4978 - val_loss: 150.0365 - val_mae: 9.1217\n",
            "Epoch 196/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 49.9960 - mae: 5.1851 - val_loss: 142.9632 - val_mae: 9.0623\n",
            "Epoch 197/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 57.7192 - mae: 5.4156 - val_loss: 147.4284 - val_mae: 9.1256\n",
            "Epoch 198/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 60.4325 - mae: 5.4022 - val_loss: 154.2411 - val_mae: 9.2260\n",
            "Epoch 199/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 58.2636 - mae: 5.3884 - val_loss: 151.3452 - val_mae: 9.2719\n",
            "Epoch 200/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52.4632 - mae: 5.3118 - val_loss: 157.2565 - val_mae: 9.4199\n",
            "MLP Train Loss (RMSE): 7.6687751303263365\n",
            "MLP Validation Loss (RMSE): 12.540194774617799, MAE: 9.41993236541748\n",
            "MLP Test Loss (RMSE): 21.759807611624737, MAE: 14.722986221313477\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = processed_data.drop(columns=['TM', 'PM10'])\n",
        "y = processed_data['PM10']\n",
        "\n",
        "# Split into training, validation, and test sets\n",
        "train_size = 0.7\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=train_size, shuffle=False)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# MLP 모델 설계\n",
        "mlp_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "mlp_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# 학습\n",
        "history = mlp_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_data=(X_val_scaled, y_val),\n",
        "    epochs=200,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 평가\n",
        "train_loss = history.history['loss'][-1]  # 마지막 epoch의 train loss\n",
        "val_loss, val_mae = mlp_model.evaluate(X_val_scaled, y_val, verbose=0)\n",
        "test_loss, test_mae = mlp_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "\n",
        "print(f\"MLP Train Loss (RMSE): {np.sqrt(train_loss)}\")\n",
        "print(f\"MLP Validation Loss (RMSE): {np.sqrt(val_loss)}, MAE: {val_mae}\")\n",
        "print(f\"MLP Test Loss (RMSE): {np.sqrt(test_loss)}, MAE: {test_mae}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}