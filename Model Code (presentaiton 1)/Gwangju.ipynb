{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJwQfT5t0txd"
      },
      "source": [
        "데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BQkNZJ5z0tjT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "file_path = 'kma_2022_gwangju_noNULL_noDuplicated.csv'  # 파일 경로를 지정하세요.\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert 'TM' column to datetime format\n",
        "data['TM'] = pd.to_datetime(data['TM'], format='%Y%m%d%H%M')\n",
        "\n",
        "# Select relevant columns for prediction\n",
        "selected_columns = ['STN','TM', 'WD', 'WS', 'PA', 'PS', 'TA', 'TD', 'HM', 'CA_TOT', 'CA_MID', 'VS', 'TS', 'TE_005', 'TE_01', 'TE_02','TE_03', 'PV', 'PM10']\n",
        "processed_data = data[selected_columns]\n",
        "\n",
        "# Sort by time\n",
        "processed_data = processed_data.sort_values('TM').reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHIeL0540El2"
      },
      "source": [
        "BASE MODEL : ARIMA 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt2mshEn0CcB",
        "outputId": "c9628620-f77a-4ff8-f7dc-512a3916be94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARIMA Validation RMSE: 40.444368449776945, MAE: 18.396029724321956\n",
            "ARIMA Test RMSE: 32.62832307024315, MAE: 19.10128552413279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
            "  return get_prediction_index(\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
            "  return get_prediction_index(\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
            "  return get_prediction_index(\n"
          ]
        }
      ],
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "arima_data = processed_data[['TM', 'PM10']].set_index('TM')\n",
        "\n",
        "train_size = int(len(arima_data) * 0.7)\n",
        "val_size = int(len(arima_data) * 0.15)\n",
        "\n",
        "train_arima = arima_data.iloc[:train_size]['PM10']\n",
        "y_val = arima_data.iloc[train_size:train_size + val_size]['PM10']\n",
        "y_test = arima_data.iloc[train_size + val_size:]['PM10']\n",
        "arima_order = (5, 1, 0)  # (p, d, q) 값 조정 가능\n",
        "arima_model = ARIMA(train_arima, order=arima_order)\n",
        "arima_fit = arima_model.fit()\n",
        "\n",
        "# 예측\n",
        "forecast_val = arima_fit.forecast(steps=len(y_val))\n",
        "forecast_test = arima_fit.forecast(steps=len(y_test))\n",
        "\n",
        "# 평가\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, forecast_val))\n",
        "val_mae = mean_absolute_error(y_val, forecast_val)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, forecast_test))\n",
        "test_mae = mean_absolute_error(y_test, forecast_test)\n",
        "\n",
        "print(f\"ARIMA Validation RMSE: {val_rmse}, MAE: {val_mae}\")\n",
        "print(f\"ARIMA Test RMSE: {test_rmse}, MAE: {test_mae}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ909Xyq0HVK"
      },
      "source": [
        "MLP model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRxHLtqX0EMu",
        "outputId": "738113a4-0e90-441a-e3dd-177540b1c58b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 710.7253 - mae: 18.7008 - val_loss: 1225.0396 - val_mae: 14.8091\n",
            "Epoch 2/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 251.9677 - mae: 10.8915 - val_loss: 1209.5156 - val_mae: 13.6332\n",
            "Epoch 3/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 244.9678 - mae: 10.4979 - val_loss: 1191.9388 - val_mae: 14.3104\n",
            "Epoch 4/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 227.9834 - mae: 10.2810 - val_loss: 1203.8357 - val_mae: 14.4899\n",
            "Epoch 5/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 219.3726 - mae: 9.9146 - val_loss: 1194.8020 - val_mae: 14.6069\n",
            "Epoch 6/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 200.9199 - mae: 9.9537 - val_loss: 1187.9938 - val_mae: 14.0758\n",
            "Epoch 7/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 215.7850 - mae: 9.9049 - val_loss: 1203.2819 - val_mae: 14.5520\n",
            "Epoch 8/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 201.3733 - mae: 9.6042 - val_loss: 1184.4318 - val_mae: 14.2856\n",
            "Epoch 9/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 189.5799 - mae: 9.5183 - val_loss: 1204.3719 - val_mae: 14.9383\n",
            "Epoch 10/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 192.5060 - mae: 9.7931 - val_loss: 1149.7190 - val_mae: 13.4688\n",
            "Epoch 11/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 181.9412 - mae: 9.3514 - val_loss: 1174.1759 - val_mae: 13.8464\n",
            "Epoch 12/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 177.4348 - mae: 9.4031 - val_loss: 1170.0948 - val_mae: 13.8243\n",
            "Epoch 13/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 177.5593 - mae: 9.1998 - val_loss: 1179.6813 - val_mae: 14.2016\n",
            "Epoch 14/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 171.3714 - mae: 9.1295 - val_loss: 1165.1025 - val_mae: 13.5860\n",
            "Epoch 15/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 183.3849 - mae: 9.3676 - val_loss: 1159.2332 - val_mae: 13.5226\n",
            "Epoch 16/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 177.7182 - mae: 9.1879 - val_loss: 1158.3331 - val_mae: 13.4576\n",
            "Epoch 17/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 177.0989 - mae: 9.2484 - val_loss: 1167.9563 - val_mae: 12.9868\n",
            "Epoch 18/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 159.8926 - mae: 9.0137 - val_loss: 1154.1794 - val_mae: 12.8705\n",
            "Epoch 19/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 165.1313 - mae: 8.9877 - val_loss: 1167.8191 - val_mae: 12.8437\n",
            "Epoch 20/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 161.6019 - mae: 8.9114 - val_loss: 1185.2651 - val_mae: 13.1824\n",
            "Epoch 21/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 165.7568 - mae: 8.8331 - val_loss: 1188.6257 - val_mae: 12.7627\n",
            "Epoch 22/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 157.1849 - mae: 9.0828 - val_loss: 1173.8406 - val_mae: 13.1090\n",
            "Epoch 23/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 161.5628 - mae: 8.8391 - val_loss: 1188.1062 - val_mae: 13.1832\n",
            "Epoch 24/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 187.4836 - mae: 8.9738 - val_loss: 1189.0320 - val_mae: 13.5208\n",
            "Epoch 25/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 168.8050 - mae: 8.9113 - val_loss: 1169.6202 - val_mae: 12.8575\n",
            "Epoch 26/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 174.4653 - mae: 8.7232 - val_loss: 1188.1599 - val_mae: 12.5606\n",
            "Epoch 27/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 164.5061 - mae: 8.7492 - val_loss: 1177.6135 - val_mae: 13.3319\n",
            "Epoch 28/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 146.2548 - mae: 8.5793 - val_loss: 1187.9899 - val_mae: 13.5910\n",
            "Epoch 29/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 147.4877 - mae: 8.7266 - val_loss: 1203.0046 - val_mae: 14.0703\n",
            "Epoch 30/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 143.6261 - mae: 8.6223 - val_loss: 1197.8245 - val_mae: 12.8134\n",
            "Epoch 31/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 137.2733 - mae: 8.3083 - val_loss: 1190.2062 - val_mae: 13.3872\n",
            "Epoch 32/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 134.5349 - mae: 8.2721 - val_loss: 1188.9303 - val_mae: 12.7505\n",
            "Epoch 33/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 145.8058 - mae: 8.3861 - val_loss: 1259.7913 - val_mae: 13.6584\n",
            "Epoch 34/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 135.7903 - mae: 8.5115 - val_loss: 1224.5864 - val_mae: 12.7653\n",
            "Epoch 35/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 139.2628 - mae: 8.2403 - val_loss: 1240.6549 - val_mae: 13.4779\n",
            "Epoch 36/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 131.4423 - mae: 8.2989 - val_loss: 1190.0925 - val_mae: 13.5099\n",
            "Epoch 37/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 133.0691 - mae: 8.2269 - val_loss: 1214.7776 - val_mae: 13.1476\n",
            "Epoch 38/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 122.9048 - mae: 8.1419 - val_loss: 1192.5330 - val_mae: 13.3767\n",
            "Epoch 39/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 182.7529 - mae: 9.4986 - val_loss: 1211.6144 - val_mae: 13.0394\n",
            "Epoch 40/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 124.4117 - mae: 8.0028 - val_loss: 1180.8147 - val_mae: 12.7615\n",
            "Epoch 41/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 129.9554 - mae: 8.2006 - val_loss: 1197.1235 - val_mae: 13.1494\n",
            "Epoch 42/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 111.4830 - mae: 7.8952 - val_loss: 1215.2961 - val_mae: 13.5613\n",
            "Epoch 43/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 124.5158 - mae: 8.1523 - val_loss: 1235.5074 - val_mae: 13.6043\n",
            "Epoch 44/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 149.2230 - mae: 8.1812 - val_loss: 1197.3250 - val_mae: 12.8682\n",
            "Epoch 45/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 126.4590 - mae: 7.9483 - val_loss: 1222.0735 - val_mae: 13.4226\n",
            "Epoch 46/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 122.2457 - mae: 7.9929 - val_loss: 1208.3435 - val_mae: 13.6866\n",
            "Epoch 47/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 126.3483 - mae: 7.9023 - val_loss: 1252.6681 - val_mae: 13.6961\n",
            "Epoch 48/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 120.7962 - mae: 8.0116 - val_loss: 1236.3695 - val_mae: 14.0738\n",
            "Epoch 49/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 132.2719 - mae: 8.1299 - val_loss: 1200.5181 - val_mae: 13.6764\n",
            "Epoch 50/50\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 117.9376 - mae: 7.9857 - val_loss: 1244.1188 - val_mae: 13.3935\n",
            "MLP Train Loss (RMSE): 11.03350201926306\n",
            "MLP Validation Loss (RMSE): 35.27206790668875, MAE: 13.39345932006836\n",
            "MLP Test Loss (RMSE): 24.337738195820112, MAE: 12.65595531463623\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = processed_data.drop(columns=['TM', 'PM10'])\n",
        "y = processed_data['PM10']\n",
        "\n",
        "# Split into training, validation, and test sets\n",
        "train_size = 0.7\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=train_size, shuffle=False)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# MLP 모델 설계\n",
        "mlp_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "mlp_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# 학습\n",
        "history = mlp_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_data=(X_val_scaled, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 평가\n",
        "train_loss = history.history['loss'][-1]  # 마지막 epoch의 train loss\n",
        "val_loss, val_mae = mlp_model.evaluate(X_val_scaled, y_val, verbose=0)\n",
        "test_loss, test_mae = mlp_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "\n",
        "print(f\"MLP Train Loss (RMSE): {np.sqrt(train_loss)}\")\n",
        "print(f\"MLP Validation Loss (RMSE): {np.sqrt(val_loss)}, MAE: {val_mae}\")\n",
        "print(f\"MLP Test Loss (RMSE): {np.sqrt(test_loss)}, MAE: {test_mae}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkV-tvBg2l2I",
        "outputId": "7daf301d-1162-46f2-877e-913cc0079409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 641.4883 - mae: 17.5288 - val_loss: 1195.7706 - val_mae: 14.2695\n",
            "Epoch 2/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 255.3756 - mae: 10.9423 - val_loss: 1210.2181 - val_mae: 14.1312\n",
            "Epoch 3/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 229.6860 - mae: 10.5193 - val_loss: 1196.2682 - val_mae: 14.8311\n",
            "Epoch 4/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 245.0534 - mae: 10.3402 - val_loss: 1186.1312 - val_mae: 14.4751\n",
            "Epoch 5/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 215.1015 - mae: 10.1141 - val_loss: 1198.4506 - val_mae: 14.1763\n",
            "Epoch 6/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 205.1563 - mae: 9.8368 - val_loss: 1188.7345 - val_mae: 13.7048\n",
            "Epoch 7/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 194.1025 - mae: 9.7389 - val_loss: 1181.1962 - val_mae: 14.1548\n",
            "Epoch 8/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 185.5320 - mae: 9.4610 - val_loss: 1181.7766 - val_mae: 14.0374\n",
            "Epoch 9/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 226.6891 - mae: 9.7345 - val_loss: 1185.3152 - val_mae: 14.0839\n",
            "Epoch 10/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 187.3681 - mae: 9.3170 - val_loss: 1165.6016 - val_mae: 13.6781\n",
            "Epoch 11/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 208.4096 - mae: 9.4516 - val_loss: 1189.6865 - val_mae: 13.3904\n",
            "Epoch 12/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 179.3231 - mae: 9.2562 - val_loss: 1164.0165 - val_mae: 13.7626\n",
            "Epoch 13/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 163.6703 - mae: 9.1376 - val_loss: 1187.4619 - val_mae: 14.2712\n",
            "Epoch 14/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 168.4956 - mae: 9.0623 - val_loss: 1157.4977 - val_mae: 13.6376\n",
            "Epoch 15/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 188.9168 - mae: 9.2929 - val_loss: 1169.6372 - val_mae: 13.6619\n",
            "Epoch 16/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 197.2739 - mae: 9.0224 - val_loss: 1157.8767 - val_mae: 13.1078\n",
            "Epoch 17/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 165.5862 - mae: 8.9640 - val_loss: 1167.7894 - val_mae: 13.8317\n",
            "Epoch 18/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 156.1901 - mae: 8.7518 - val_loss: 1177.0576 - val_mae: 13.3556\n",
            "Epoch 19/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 180.2536 - mae: 9.1056 - val_loss: 1167.8311 - val_mae: 13.5881\n",
            "Epoch 20/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 166.2352 - mae: 8.7741 - val_loss: 1220.0831 - val_mae: 14.1572\n",
            "Epoch 21/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 165.5871 - mae: 8.9379 - val_loss: 1193.8173 - val_mae: 13.3369\n",
            "Epoch 22/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 170.6573 - mae: 8.9024 - val_loss: 1185.1979 - val_mae: 13.4149\n",
            "Epoch 23/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 162.2372 - mae: 8.9164 - val_loss: 1176.3048 - val_mae: 13.2161\n",
            "Epoch 24/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 160.1745 - mae: 8.7855 - val_loss: 1234.1218 - val_mae: 13.8214\n",
            "Epoch 25/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 146.6997 - mae: 8.6789 - val_loss: 1200.1497 - val_mae: 13.5444\n",
            "Epoch 26/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 137.6185 - mae: 8.4388 - val_loss: 1202.3070 - val_mae: 13.3731\n",
            "Epoch 27/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 160.2760 - mae: 8.6998 - val_loss: 1195.5615 - val_mae: 13.4507\n",
            "Epoch 28/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 134.8311 - mae: 8.2289 - val_loss: 1195.9144 - val_mae: 13.4354\n",
            "Epoch 29/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 148.0020 - mae: 8.6083 - val_loss: 1197.6763 - val_mae: 13.2104\n",
            "Epoch 30/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 126.0057 - mae: 8.1536 - val_loss: 1207.6719 - val_mae: 13.4393\n",
            "Epoch 31/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 147.8034 - mae: 8.5258 - val_loss: 1243.2892 - val_mae: 13.9030\n",
            "Epoch 32/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 164.5749 - mae: 8.7659 - val_loss: 1199.6752 - val_mae: 13.1524\n",
            "Epoch 33/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 134.9476 - mae: 8.4597 - val_loss: 1215.2642 - val_mae: 13.6313\n",
            "Epoch 34/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 136.0454 - mae: 8.2751 - val_loss: 1197.3077 - val_mae: 13.3580\n",
            "Epoch 35/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 128.6516 - mae: 8.1450 - val_loss: 1186.3058 - val_mae: 13.3536\n",
            "Epoch 36/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 134.1395 - mae: 8.1148 - val_loss: 1216.6754 - val_mae: 13.7342\n",
            "Epoch 37/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 129.3545 - mae: 8.2638 - val_loss: 1229.7665 - val_mae: 13.6330\n",
            "Epoch 38/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 139.9011 - mae: 8.3373 - val_loss: 1248.1272 - val_mae: 13.6479\n",
            "Epoch 39/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 124.2207 - mae: 8.0906 - val_loss: 1222.3300 - val_mae: 13.8000\n",
            "Epoch 40/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 136.0877 - mae: 8.2645 - val_loss: 1216.3682 - val_mae: 14.2512\n",
            "Epoch 41/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 136.4750 - mae: 8.2837 - val_loss: 1243.0439 - val_mae: 14.0708\n",
            "Epoch 42/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 139.1855 - mae: 8.1630 - val_loss: 1235.0793 - val_mae: 14.1074\n",
            "Epoch 43/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 133.8865 - mae: 8.0879 - val_loss: 1212.3195 - val_mae: 13.5186\n",
            "Epoch 44/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 119.5894 - mae: 7.8664 - val_loss: 1218.6179 - val_mae: 13.8806\n",
            "Epoch 45/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 148.4649 - mae: 8.1253 - val_loss: 1235.1311 - val_mae: 13.2213\n",
            "Epoch 46/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 120.9987 - mae: 8.0658 - val_loss: 1260.2139 - val_mae: 14.0924\n",
            "Epoch 47/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 113.1297 - mae: 7.8420 - val_loss: 1245.9941 - val_mae: 13.4143\n",
            "Epoch 48/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 131.4209 - mae: 8.0656 - val_loss: 1235.6243 - val_mae: 13.4859\n",
            "Epoch 49/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 116.3290 - mae: 7.9349 - val_loss: 1251.2861 - val_mae: 14.1105\n",
            "Epoch 50/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 114.9294 - mae: 7.7603 - val_loss: 1224.1313 - val_mae: 13.5783\n",
            "Epoch 51/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 131.3925 - mae: 7.8928 - val_loss: 1219.0371 - val_mae: 13.8112\n",
            "Epoch 52/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 107.2489 - mae: 7.7298 - val_loss: 1237.2015 - val_mae: 13.7799\n",
            "Epoch 53/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 111.5059 - mae: 7.8573 - val_loss: 1217.1321 - val_mae: 13.4745\n",
            "Epoch 54/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 107.2747 - mae: 7.6892 - val_loss: 1228.1995 - val_mae: 14.0057\n",
            "Epoch 55/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 120.7844 - mae: 7.7173 - val_loss: 1229.3683 - val_mae: 13.5758\n",
            "Epoch 56/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 128.1215 - mae: 7.6887 - val_loss: 1221.5577 - val_mae: 13.7177\n",
            "Epoch 57/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 101.6641 - mae: 7.4564 - val_loss: 1222.7158 - val_mae: 13.9588\n",
            "Epoch 58/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 106.7544 - mae: 7.6745 - val_loss: 1244.7622 - val_mae: 14.6427\n",
            "Epoch 59/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 107.5691 - mae: 7.7520 - val_loss: 1222.4388 - val_mae: 14.1052\n",
            "Epoch 60/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 111.6972 - mae: 7.8642 - val_loss: 1224.1666 - val_mae: 13.5037\n",
            "Epoch 61/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 103.9373 - mae: 7.4742 - val_loss: 1251.2080 - val_mae: 13.8692\n",
            "Epoch 62/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 113.1109 - mae: 7.8174 - val_loss: 1245.8702 - val_mae: 14.1097\n",
            "Epoch 63/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 107.4753 - mae: 7.6192 - val_loss: 1243.8798 - val_mae: 14.6004\n",
            "Epoch 64/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 136.1508 - mae: 8.3265 - val_loss: 1226.4740 - val_mae: 13.9650\n",
            "Epoch 65/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 106.2815 - mae: 7.5203 - val_loss: 1240.9694 - val_mae: 13.9154\n",
            "Epoch 66/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 103.2657 - mae: 7.6146 - val_loss: 1222.1967 - val_mae: 13.7466\n",
            "Epoch 67/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 112.6211 - mae: 7.5736 - val_loss: 1233.4303 - val_mae: 14.8642\n",
            "Epoch 68/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 105.4125 - mae: 7.5021 - val_loss: 1240.7545 - val_mae: 14.1904\n",
            "Epoch 69/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 109.6206 - mae: 7.3989 - val_loss: 1221.5491 - val_mae: 13.6356\n",
            "Epoch 70/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 97.7325 - mae: 7.4061 - val_loss: 1225.2686 - val_mae: 13.6161\n",
            "Epoch 71/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 99.4748 - mae: 7.4019 - val_loss: 1213.7760 - val_mae: 14.1411\n",
            "Epoch 72/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 100.4815 - mae: 7.3446 - val_loss: 1229.6754 - val_mae: 14.3316\n",
            "Epoch 73/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 123.0233 - mae: 7.8940 - val_loss: 1194.6971 - val_mae: 14.0497\n",
            "Epoch 74/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 112.6132 - mae: 7.5601 - val_loss: 1215.0211 - val_mae: 13.6805\n",
            "Epoch 75/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 109.9538 - mae: 7.2047 - val_loss: 1245.1946 - val_mae: 14.3956\n",
            "Epoch 76/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 101.4936 - mae: 7.4909 - val_loss: 1229.7571 - val_mae: 13.8319\n",
            "Epoch 77/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 100.6814 - mae: 7.2645 - val_loss: 1235.4972 - val_mae: 14.4380\n",
            "Epoch 78/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 88.2673 - mae: 7.1216 - val_loss: 1228.0895 - val_mae: 14.2065\n",
            "Epoch 79/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 91.6954 - mae: 7.1859 - val_loss: 1225.9015 - val_mae: 13.5572\n",
            "Epoch 80/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 116.6343 - mae: 7.6334 - val_loss: 1258.8777 - val_mae: 14.0269\n",
            "Epoch 81/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 97.9384 - mae: 7.3063 - val_loss: 1227.3882 - val_mae: 13.6027\n",
            "Epoch 82/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 99.6966 - mae: 7.1211 - val_loss: 1233.3585 - val_mae: 14.3395\n",
            "Epoch 83/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 100.5134 - mae: 7.2869 - val_loss: 1239.0864 - val_mae: 13.8246\n",
            "Epoch 84/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 106.9055 - mae: 7.2612 - val_loss: 1231.4398 - val_mae: 13.6674\n",
            "Epoch 85/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 101.2548 - mae: 7.4582 - val_loss: 1254.2605 - val_mae: 14.1456\n",
            "Epoch 86/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 94.2484 - mae: 7.2171 - val_loss: 1259.5598 - val_mae: 14.4285\n",
            "Epoch 87/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 113.1206 - mae: 7.3661 - val_loss: 1223.1838 - val_mae: 13.9270\n",
            "Epoch 88/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 94.4692 - mae: 7.0738 - val_loss: 1205.4164 - val_mae: 13.9224\n",
            "Epoch 89/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 89.2340 - mae: 7.0323 - val_loss: 1212.6678 - val_mae: 14.0450\n",
            "Epoch 90/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 103.9790 - mae: 7.2176 - val_loss: 1231.4094 - val_mae: 14.1059\n",
            "Epoch 91/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 107.2485 - mae: 7.3326 - val_loss: 1222.7960 - val_mae: 13.8044\n",
            "Epoch 92/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 87.2719 - mae: 6.9171 - val_loss: 1222.9561 - val_mae: 13.6787\n",
            "Epoch 93/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 119.1830 - mae: 7.2300 - val_loss: 1255.9332 - val_mae: 14.3383\n",
            "Epoch 94/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 95.6090 - mae: 7.1739 - val_loss: 1237.6732 - val_mae: 14.2747\n",
            "Epoch 95/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 96.0632 - mae: 7.0775 - val_loss: 1238.5942 - val_mae: 14.1163\n",
            "Epoch 96/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 93.6110 - mae: 7.0928 - val_loss: 1230.5538 - val_mae: 14.0398\n",
            "Epoch 97/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 100.5126 - mae: 7.0440 - val_loss: 1221.7006 - val_mae: 13.6732\n",
            "Epoch 98/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 94.6444 - mae: 7.0723 - val_loss: 1243.6111 - val_mae: 14.3876\n",
            "Epoch 99/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 100.2446 - mae: 7.0456 - val_loss: 1219.0100 - val_mae: 14.1605\n",
            "Epoch 100/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 131.7566 - mae: 7.7825 - val_loss: 1243.9750 - val_mae: 14.1066\n",
            "Epoch 101/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 88.2829 - mae: 6.8805 - val_loss: 1251.7209 - val_mae: 14.5022\n",
            "Epoch 102/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 104.4686 - mae: 6.8698 - val_loss: 1224.6168 - val_mae: 13.8616\n",
            "Epoch 103/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 97.3366 - mae: 7.0205 - val_loss: 1240.8599 - val_mae: 14.1422\n",
            "Epoch 104/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 81.5840 - mae: 6.8368 - val_loss: 1258.3083 - val_mae: 14.3149\n",
            "Epoch 105/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 97.9524 - mae: 7.0828 - val_loss: 1221.9479 - val_mae: 14.0425\n",
            "Epoch 106/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 95.1968 - mae: 6.8556 - val_loss: 1237.0315 - val_mae: 13.9087\n",
            "Epoch 107/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 86.2617 - mae: 6.7427 - val_loss: 1256.1150 - val_mae: 14.3798\n",
            "Epoch 108/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 106.7127 - mae: 7.1348 - val_loss: 1247.2854 - val_mae: 14.1253\n",
            "Epoch 109/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 85.7428 - mae: 6.8836 - val_loss: 1235.7549 - val_mae: 13.8270\n",
            "Epoch 110/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 94.2387 - mae: 6.8479 - val_loss: 1222.5845 - val_mae: 14.4644\n",
            "Epoch 111/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 97.4320 - mae: 6.8410 - val_loss: 1261.8998 - val_mae: 14.3602\n",
            "Epoch 112/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 88.5510 - mae: 6.7865 - val_loss: 1268.1556 - val_mae: 14.3207\n",
            "Epoch 113/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 104.6245 - mae: 6.9834 - val_loss: 1221.6333 - val_mae: 13.7389\n",
            "Epoch 114/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 91.6842 - mae: 6.8592 - val_loss: 1238.8423 - val_mae: 14.7165\n",
            "Epoch 115/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 145.0085 - mae: 8.2045 - val_loss: 1231.0767 - val_mae: 14.2703\n",
            "Epoch 116/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 80.0777 - mae: 6.7256 - val_loss: 1232.2047 - val_mae: 14.5547\n",
            "Epoch 117/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 78.1585 - mae: 6.6702 - val_loss: 1283.2346 - val_mae: 14.7866\n",
            "Epoch 118/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 88.9246 - mae: 6.7646 - val_loss: 1221.0369 - val_mae: 13.9356\n",
            "Epoch 119/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 89.1064 - mae: 6.8069 - val_loss: 1250.2281 - val_mae: 14.0692\n",
            "Epoch 120/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 88.8860 - mae: 6.7709 - val_loss: 1233.9877 - val_mae: 14.1421\n",
            "Epoch 121/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 88.7121 - mae: 6.8818 - val_loss: 1275.7249 - val_mae: 14.9499\n",
            "Epoch 122/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 81.1931 - mae: 6.7399 - val_loss: 1218.2340 - val_mae: 14.2436\n",
            "Epoch 123/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 91.5498 - mae: 6.8378 - val_loss: 1250.1799 - val_mae: 15.2615\n",
            "Epoch 124/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 97.5807 - mae: 6.9378 - val_loss: 1276.9739 - val_mae: 14.9226\n",
            "Epoch 125/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 83.8941 - mae: 6.5740 - val_loss: 1253.2087 - val_mae: 14.3165\n",
            "Epoch 126/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 89.3951 - mae: 6.7045 - val_loss: 1242.6202 - val_mae: 14.7337\n",
            "Epoch 127/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 88.7264 - mae: 6.7964 - val_loss: 1257.7042 - val_mae: 14.7294\n",
            "Epoch 128/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 93.3472 - mae: 6.7640 - val_loss: 1252.3236 - val_mae: 14.9094\n",
            "Epoch 129/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 76.5481 - mae: 6.6014 - val_loss: 1252.9282 - val_mae: 14.4776\n",
            "Epoch 130/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 78.3565 - mae: 6.5602 - val_loss: 1240.6838 - val_mae: 14.5372\n",
            "Epoch 131/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 83.8414 - mae: 6.5194 - val_loss: 1235.2844 - val_mae: 14.8845\n",
            "Epoch 132/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 96.6471 - mae: 6.6522 - val_loss: 1236.5588 - val_mae: 13.9993\n",
            "Epoch 133/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 90.1708 - mae: 6.6768 - val_loss: 1234.9323 - val_mae: 14.3860\n",
            "Epoch 134/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85.4622 - mae: 6.6179 - val_loss: 1246.2969 - val_mae: 14.6213\n",
            "Epoch 135/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 90.9613 - mae: 6.7793 - val_loss: 1264.7635 - val_mae: 14.5372\n",
            "Epoch 136/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 82.8689 - mae: 6.6189 - val_loss: 1244.3921 - val_mae: 14.2542\n",
            "Epoch 137/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 75.2146 - mae: 6.5700 - val_loss: 1237.5087 - val_mae: 14.7897\n",
            "Epoch 138/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 83.3765 - mae: 6.6801 - val_loss: 1242.6606 - val_mae: 14.0563\n",
            "Epoch 139/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 83.2021 - mae: 6.6326 - val_loss: 1264.7987 - val_mae: 15.0524\n",
            "Epoch 140/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 82.0428 - mae: 6.5481 - val_loss: 1277.0334 - val_mae: 15.2227\n",
            "Epoch 141/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 79.3508 - mae: 6.6385 - val_loss: 1253.2852 - val_mae: 14.4274\n",
            "Epoch 142/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 81.2132 - mae: 6.5348 - val_loss: 1271.7531 - val_mae: 14.1876\n",
            "Epoch 143/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 95.5358 - mae: 6.6764 - val_loss: 1259.0244 - val_mae: 15.3444\n",
            "Epoch 144/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.2892 - mae: 6.4969 - val_loss: 1253.0162 - val_mae: 14.2523\n",
            "Epoch 145/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 88.7528 - mae: 6.6400 - val_loss: 1245.9099 - val_mae: 15.0587\n",
            "Epoch 146/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 88.6952 - mae: 6.5677 - val_loss: 1251.9862 - val_mae: 14.4016\n",
            "Epoch 147/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 81.0561 - mae: 6.4103 - val_loss: 1247.3373 - val_mae: 14.3084\n",
            "Epoch 148/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 96.4853 - mae: 6.6630 - val_loss: 1225.4731 - val_mae: 14.2202\n",
            "Epoch 149/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 69.8077 - mae: 6.2363 - val_loss: 1252.7734 - val_mae: 14.7435\n",
            "Epoch 150/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.0729 - mae: 6.4081 - val_loss: 1280.1582 - val_mae: 15.5871\n",
            "Epoch 151/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 77.7234 - mae: 6.5183 - val_loss: 1247.2340 - val_mae: 14.5186\n",
            "Epoch 152/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 94.1487 - mae: 6.4747 - val_loss: 1240.8789 - val_mae: 14.5226\n",
            "Epoch 153/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 72.4131 - mae: 6.2492 - val_loss: 1276.5087 - val_mae: 14.6027\n",
            "Epoch 154/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 80.4020 - mae: 6.3847 - val_loss: 1292.4381 - val_mae: 15.2060\n",
            "Epoch 155/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 75.9554 - mae: 6.3896 - val_loss: 1249.0817 - val_mae: 15.1069\n",
            "Epoch 156/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 74.6032 - mae: 6.4580 - val_loss: 1321.0980 - val_mae: 15.5328\n",
            "Epoch 157/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 98.2867 - mae: 7.1356 - val_loss: 1222.3276 - val_mae: 14.6849\n",
            "Epoch 158/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 81.4258 - mae: 6.4877 - val_loss: 1241.4142 - val_mae: 14.9289\n",
            "Epoch 159/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 66.9218 - mae: 6.1125 - val_loss: 1232.0731 - val_mae: 14.8660\n",
            "Epoch 160/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 76.1935 - mae: 6.3743 - val_loss: 1252.0131 - val_mae: 14.3452\n",
            "Epoch 161/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.8033 - mae: 6.2300 - val_loss: 1229.3762 - val_mae: 14.4199\n",
            "Epoch 162/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 72.8640 - mae: 6.2640 - val_loss: 1219.7587 - val_mae: 14.0850\n",
            "Epoch 163/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 102.0038 - mae: 7.1368 - val_loss: 1221.9888 - val_mae: 14.8797\n",
            "Epoch 164/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78.1852 - mae: 6.3664 - val_loss: 1244.3206 - val_mae: 14.7299\n",
            "Epoch 165/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 76.6207 - mae: 6.2574 - val_loss: 1269.6108 - val_mae: 14.7276\n",
            "Epoch 166/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 77.6870 - mae: 6.6107 - val_loss: 1275.6908 - val_mae: 14.7325\n",
            "Epoch 167/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 84.7048 - mae: 6.3733 - val_loss: 1232.2262 - val_mae: 14.2626\n",
            "Epoch 168/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 67.8488 - mae: 6.0916 - val_loss: 1248.2456 - val_mae: 14.4256\n",
            "Epoch 169/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.2007 - mae: 6.1350 - val_loss: 1239.4501 - val_mae: 14.7129\n",
            "Epoch 170/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 68.1995 - mae: 6.1655 - val_loss: 1232.8145 - val_mae: 14.2780\n",
            "Epoch 171/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.1758 - mae: 6.1288 - val_loss: 1232.1625 - val_mae: 14.4879\n",
            "Epoch 172/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 68.0523 - mae: 6.0796 - val_loss: 1270.8073 - val_mae: 15.3577\n",
            "Epoch 173/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 79.1780 - mae: 6.3217 - val_loss: 1261.9736 - val_mae: 14.3570\n",
            "Epoch 174/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 74.8889 - mae: 6.3317 - val_loss: 1266.9390 - val_mae: 14.6130\n",
            "Epoch 175/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 73.4680 - mae: 6.2477 - val_loss: 1242.8546 - val_mae: 14.6213\n",
            "Epoch 176/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 91.8769 - mae: 6.5789 - val_loss: 1279.1267 - val_mae: 15.4119\n",
            "Epoch 177/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 74.8007 - mae: 6.2099 - val_loss: 1267.4633 - val_mae: 14.6105\n",
            "Epoch 178/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 69.2831 - mae: 6.1443 - val_loss: 1270.8157 - val_mae: 15.1051\n",
            "Epoch 179/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 80.2402 - mae: 6.3909 - val_loss: 1247.9330 - val_mae: 14.5859\n",
            "Epoch 180/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 74.4869 - mae: 6.2130 - val_loss: 1263.0806 - val_mae: 14.9828\n",
            "Epoch 181/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 75.1849 - mae: 6.2514 - val_loss: 1263.1025 - val_mae: 15.1514\n",
            "Epoch 182/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78.1475 - mae: 6.1593 - val_loss: 1270.1721 - val_mae: 15.3151\n",
            "Epoch 183/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.1908 - mae: 6.2293 - val_loss: 1261.4556 - val_mae: 14.7450\n",
            "Epoch 184/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.5944 - mae: 6.2583 - val_loss: 1258.8448 - val_mae: 14.8705\n",
            "Epoch 185/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78.7310 - mae: 6.1295 - val_loss: 1231.5641 - val_mae: 14.5150\n",
            "Epoch 186/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 68.7768 - mae: 5.9903 - val_loss: 1265.1943 - val_mae: 14.5112\n",
            "Epoch 187/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 66.2396 - mae: 6.0328 - val_loss: 1285.3191 - val_mae: 15.1640\n",
            "Epoch 188/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 71.9583 - mae: 6.4096 - val_loss: 1286.6180 - val_mae: 15.1810\n",
            "Epoch 189/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.2755 - mae: 6.2614 - val_loss: 1244.4880 - val_mae: 14.4500\n",
            "Epoch 190/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 68.7615 - mae: 6.1028 - val_loss: 1267.6287 - val_mae: 14.9903\n",
            "Epoch 191/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 67.7355 - mae: 6.0160 - val_loss: 1273.1500 - val_mae: 14.9331\n",
            "Epoch 192/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 76.0902 - mae: 6.1648 - val_loss: 1270.8823 - val_mae: 15.1553\n",
            "Epoch 193/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 69.0347 - mae: 6.0182 - val_loss: 1259.4016 - val_mae: 14.4665\n",
            "Epoch 194/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 86.8670 - mae: 6.2864 - val_loss: 1220.4426 - val_mae: 14.3964\n",
            "Epoch 195/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 70.2096 - mae: 6.0419 - val_loss: 1261.7236 - val_mae: 15.1521\n",
            "Epoch 196/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 70.4249 - mae: 5.9716 - val_loss: 1223.1876 - val_mae: 14.3032\n",
            "Epoch 197/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 66.1525 - mae: 6.0793 - val_loss: 1279.5447 - val_mae: 15.3412\n",
            "Epoch 198/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 71.7558 - mae: 6.1673 - val_loss: 1251.0785 - val_mae: 14.9099\n",
            "Epoch 199/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 63.9061 - mae: 5.9178 - val_loss: 1253.7548 - val_mae: 14.2757\n",
            "Epoch 200/200\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 62.0598 - mae: 5.8728 - val_loss: 1287.1454 - val_mae: 15.2271\n",
            "MLP Train Loss (RMSE): 8.438874309371778\n",
            "MLP Validation Loss (RMSE): 35.87680846650364, MAE: 15.22706413269043\n",
            "MLP Test Loss (RMSE): 25.69240135340968, MAE: 14.031721115112305\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = processed_data.drop(columns=['TM', 'PM10'])\n",
        "y = processed_data['PM10']\n",
        "\n",
        "# Split into training, validation, and test sets\n",
        "train_size = 0.7\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=train_size, shuffle=False)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# MLP 모델 설계\n",
        "mlp_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "mlp_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# 학습\n",
        "history = mlp_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_data=(X_val_scaled, y_val),\n",
        "    epochs=200,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 평가\n",
        "train_loss = history.history['loss'][-1]  # 마지막 epoch의 train loss\n",
        "val_loss, val_mae = mlp_model.evaluate(X_val_scaled, y_val, verbose=0)\n",
        "test_loss, test_mae = mlp_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "\n",
        "print(f\"MLP Train Loss (RMSE): {np.sqrt(train_loss)}\")\n",
        "print(f\"MLP Validation Loss (RMSE): {np.sqrt(val_loss)}, MAE: {val_mae}\")\n",
        "print(f\"MLP Test Loss (RMSE): {np.sqrt(test_loss)}, MAE: {test_mae}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}