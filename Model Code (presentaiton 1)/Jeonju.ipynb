{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJwQfT5t0txd"
      },
      "source": [
        "데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BQkNZJ5z0tjT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "file_path = 'kma_2022_jeonju_noNULL_noDuplicated.csv'  # 파일 경로를 지정하세요.\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert 'TM' column to datetime format\n",
        "data['TM'] = pd.to_datetime(data['TM'], format='%Y%m%d%H%M')\n",
        "\n",
        "# Select relevant columns for prediction\n",
        "selected_columns = ['STN','TM', 'WD', 'WS', 'PA', 'PS', 'TA', 'TD', 'HM', 'CA_TOT', 'CA_MID', 'VS', 'TS', 'TE_005', 'TE_01', 'TE_02','TE_03', 'PV', 'PM10']\n",
        "processed_data = data[selected_columns]\n",
        "\n",
        "# Sort by time\n",
        "processed_data = processed_data.sort_values('TM').reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHIeL0540El2"
      },
      "source": [
        "BASE MODEL : ARIMA 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt2mshEn0CcB",
        "outputId": "42bde2cc-1b1f-4886-f045-f53e59176d60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARIMA Validation RMSE: 19.229282851013295, MAE: 14.400333205730684\n",
            "ARIMA Test RMSE: 40.08816092897648, MAE: 21.46275262683131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
            "  return get_prediction_index(\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
            "  return get_prediction_index(\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
            "  return get_prediction_index(\n"
          ]
        }
      ],
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "arima_data = processed_data[['TM', 'PM10']].set_index('TM')\n",
        "\n",
        "train_size = int(len(arima_data) * 0.7)\n",
        "val_size = int(len(arima_data) * 0.15)\n",
        "\n",
        "train_arima = arima_data.iloc[:train_size]['PM10']\n",
        "y_val = arima_data.iloc[train_size:train_size + val_size]['PM10']\n",
        "y_test = arima_data.iloc[train_size + val_size:]['PM10']\n",
        "arima_order = (5, 1, 0)  # (p, d, q) 값 조정 가능\n",
        "arima_model = ARIMA(train_arima, order=arima_order)\n",
        "arima_fit = arima_model.fit()\n",
        "\n",
        "# 예측\n",
        "forecast_val = arima_fit.forecast(steps=len(y_val))\n",
        "forecast_test = arima_fit.forecast(steps=len(y_test))\n",
        "\n",
        "# 평가\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, forecast_val))\n",
        "val_mae = mean_absolute_error(y_val, forecast_val)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, forecast_test))\n",
        "test_mae = mean_absolute_error(y_test, forecast_test)\n",
        "\n",
        "print(f\"ARIMA Validation RMSE: {val_rmse}, MAE: {val_mae}\")\n",
        "print(f\"ARIMA Test RMSE: {test_rmse}, MAE: {test_mae}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ909Xyq0HVK"
      },
      "source": [
        "MLP model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRxHLtqX0EMu",
        "outputId": "a7ad1cdd-73b1-4365-a7a3-0c90c7d71237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 2126.9021 - mae: 22.9321 - val_loss: 261.5714 - val_mae: 12.8292\n",
            "Epoch 2/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3115.7236 - mae: 17.7733 - val_loss: 229.6223 - val_mae: 11.8456\n",
            "Epoch 3/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1666.1584 - mae: 15.3011 - val_loss: 257.2438 - val_mae: 12.2772\n",
            "Epoch 4/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1675.6478 - mae: 14.1495 - val_loss: 278.3853 - val_mae: 12.7191\n",
            "Epoch 5/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3710.1223 - mae: 16.1684 - val_loss: 253.3372 - val_mae: 12.1121\n",
            "Epoch 6/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1050.4406 - mae: 14.2927 - val_loss: 416.5720 - val_mae: 15.3861\n",
            "Epoch 7/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2830.1946 - mae: 16.5094 - val_loss: 256.3485 - val_mae: 12.0698\n",
            "Epoch 8/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1423.7238 - mae: 14.2730 - val_loss: 284.1098 - val_mae: 12.6238\n",
            "Epoch 9/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1546.9880 - mae: 13.7486 - val_loss: 306.1781 - val_mae: 12.9993\n",
            "Epoch 10/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2666.0518 - mae: 15.0190 - val_loss: 263.5554 - val_mae: 12.1419\n",
            "Epoch 11/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2786.6980 - mae: 14.0082 - val_loss: 265.8462 - val_mae: 12.2095\n",
            "Epoch 12/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2362.3762 - mae: 14.3742 - val_loss: 251.2432 - val_mae: 11.8235\n",
            "Epoch 13/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3046.6929 - mae: 14.6069 - val_loss: 261.1708 - val_mae: 11.9661\n",
            "Epoch 14/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1397.8519 - mae: 13.6626 - val_loss: 272.2544 - val_mae: 11.9782\n",
            "Epoch 15/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2621.1870 - mae: 14.4187 - val_loss: 281.5942 - val_mae: 12.3987\n",
            "Epoch 16/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2196.7961 - mae: 14.1554 - val_loss: 286.2115 - val_mae: 12.4418\n",
            "Epoch 17/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3518.8406 - mae: 15.2528 - val_loss: 295.7987 - val_mae: 12.7170\n",
            "Epoch 18/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2021.0543 - mae: 14.6368 - val_loss: 268.9887 - val_mae: 12.0554\n",
            "Epoch 19/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1659.7073 - mae: 13.3908 - val_loss: 332.7495 - val_mae: 13.0201\n",
            "Epoch 20/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3176.8318 - mae: 14.7303 - val_loss: 285.5766 - val_mae: 12.2461\n",
            "Epoch 21/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1323.8898 - mae: 13.1856 - val_loss: 345.4995 - val_mae: 13.1361\n",
            "Epoch 22/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2955.3882 - mae: 15.0227 - val_loss: 312.8813 - val_mae: 12.4227\n",
            "Epoch 23/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2281.1240 - mae: 13.9886 - val_loss: 293.8583 - val_mae: 12.2818\n",
            "Epoch 24/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1331.7502 - mae: 13.1634 - val_loss: 292.3830 - val_mae: 11.9953\n",
            "Epoch 25/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1496.3385 - mae: 13.6133 - val_loss: 300.5603 - val_mae: 12.3488\n",
            "Epoch 26/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2262.6489 - mae: 14.0963 - val_loss: 297.9980 - val_mae: 12.0458\n",
            "Epoch 27/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 982.3264 - mae: 13.0655 - val_loss: 457.3544 - val_mae: 14.1350\n",
            "Epoch 28/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2360.5508 - mae: 14.7779 - val_loss: 312.3077 - val_mae: 12.5317\n",
            "Epoch 29/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3313.7166 - mae: 14.4770 - val_loss: 361.1679 - val_mae: 12.9746\n",
            "Epoch 30/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1111.4989 - mae: 13.2832 - val_loss: 318.0664 - val_mae: 12.0431\n",
            "Epoch 31/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1806.3751 - mae: 13.7886 - val_loss: 370.4486 - val_mae: 13.2358\n",
            "Epoch 32/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1892.4783 - mae: 14.1783 - val_loss: 411.1050 - val_mae: 12.8889\n",
            "Epoch 33/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2204.9966 - mae: 13.8407 - val_loss: 364.1602 - val_mae: 13.0384\n",
            "Epoch 34/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 900.6677 - mae: 13.1412 - val_loss: 732.9934 - val_mae: 17.4128\n",
            "Epoch 35/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2486.4290 - mae: 15.0037 - val_loss: 350.3802 - val_mae: 12.7681\n",
            "Epoch 36/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2320.6929 - mae: 13.6353 - val_loss: 380.9055 - val_mae: 12.9228\n",
            "Epoch 37/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 5985.8574 - mae: 15.7763 - val_loss: 321.0951 - val_mae: 12.2677\n",
            "Epoch 38/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1054.3729 - mae: 12.6078 - val_loss: 389.2297 - val_mae: 12.3501\n",
            "Epoch 39/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1336.5065 - mae: 14.2842 - val_loss: 486.6042 - val_mae: 12.7865\n",
            "Epoch 40/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2054.1614 - mae: 14.5886 - val_loss: 319.8254 - val_mae: 12.4590\n",
            "Epoch 41/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2356.2029 - mae: 14.6269 - val_loss: 344.7312 - val_mae: 12.4638\n",
            "Epoch 42/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 692.8337 - mae: 11.9858 - val_loss: 489.7775 - val_mae: 13.3936\n",
            "Epoch 43/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1357.1324 - mae: 13.7805 - val_loss: 461.5536 - val_mae: 13.2011\n",
            "Epoch 44/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1064.9738 - mae: 12.3813 - val_loss: 443.4769 - val_mae: 13.0512\n",
            "Epoch 45/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4529.2847 - mae: 15.3227 - val_loss: 399.5677 - val_mae: 12.3799\n",
            "Epoch 46/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1694.3872 - mae: 13.1963 - val_loss: 429.5387 - val_mae: 12.6065\n",
            "Epoch 47/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 790.4888 - mae: 12.2651 - val_loss: 365.3953 - val_mae: 11.7621\n",
            "Epoch 48/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1026.3990 - mae: 12.7848 - val_loss: 544.7779 - val_mae: 13.7441\n",
            "Epoch 49/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1263.9180 - mae: 13.4445 - val_loss: 355.5919 - val_mae: 12.5693\n",
            "Epoch 50/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1575.7839 - mae: 13.4545 - val_loss: 479.8006 - val_mae: 12.6506\n",
            "MLP Train Loss (RMSE): 39.62737356110751\n",
            "MLP Validation Loss (RMSE): 21.90435182017741, MAE: 12.65060043334961\n",
            "MLP Test Loss (RMSE): 32.618311170764386, MAE: 15.799858093261719\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = processed_data.drop(columns=['TM', 'PM10'])\n",
        "y = processed_data['PM10']\n",
        "\n",
        "# Split into training, validation, and test sets\n",
        "train_size = 0.7\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=train_size, shuffle=False)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# MLP 모델 설계\n",
        "mlp_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "mlp_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# 학습\n",
        "history = mlp_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_data=(X_val_scaled, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 평가\n",
        "train_loss = history.history['loss'][-1]  # 마지막 epoch의 train loss\n",
        "val_loss, val_mae = mlp_model.evaluate(X_val_scaled, y_val, verbose=0)\n",
        "test_loss, test_mae = mlp_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "\n",
        "print(f\"MLP Train Loss (RMSE): {np.sqrt(train_loss)}\")\n",
        "print(f\"MLP Validation Loss (RMSE): {np.sqrt(val_loss)}, MAE: {val_mae}\")\n",
        "print(f\"MLP Test Loss (RMSE): {np.sqrt(test_loss)}, MAE: {test_mae}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkV-tvBg2l2I",
        "outputId": "da933d5a-f32c-4a12-e382-fbc53b6722a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 2021.1926 - mae: 24.2969 - val_loss: 237.1064 - val_mae: 12.1904\n",
            "Epoch 2/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1348.8230 - mae: 15.9765 - val_loss: 359.1743 - val_mae: 15.1030\n",
            "Epoch 3/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2598.8486 - mae: 15.9338 - val_loss: 242.7804 - val_mae: 12.0883\n",
            "Epoch 4/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3901.7090 - mae: 15.9828 - val_loss: 234.1310 - val_mae: 11.6894\n",
            "Epoch 5/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2132.0701 - mae: 14.1598 - val_loss: 266.6599 - val_mae: 12.5368\n",
            "Epoch 6/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1536.3081 - mae: 15.4650 - val_loss: 263.6764 - val_mae: 12.2557\n",
            "Epoch 7/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1353.1863 - mae: 13.8054 - val_loss: 292.7709 - val_mae: 12.8521\n",
            "Epoch 8/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2439.2483 - mae: 14.6733 - val_loss: 244.3897 - val_mae: 11.7533\n",
            "Epoch 9/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1286.8774 - mae: 13.0961 - val_loss: 288.8246 - val_mae: 12.7348\n",
            "Epoch 10/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2510.8201 - mae: 14.2767 - val_loss: 265.2237 - val_mae: 12.1787\n",
            "Epoch 11/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5844.9233 - mae: 17.3394 - val_loss: 245.8970 - val_mae: 11.7526\n",
            "Epoch 12/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 961.0250 - mae: 13.1220 - val_loss: 419.2245 - val_mae: 15.0625\n",
            "Epoch 13/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1890.8416 - mae: 14.1880 - val_loss: 262.9324 - val_mae: 12.0225\n",
            "Epoch 14/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2097.7966 - mae: 14.1208 - val_loss: 288.1651 - val_mae: 12.4813\n",
            "Epoch 15/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1019.2696 - mae: 14.0009 - val_loss: 334.3087 - val_mae: 13.6199\n",
            "Epoch 16/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1699.1523 - mae: 15.0020 - val_loss: 275.7795 - val_mae: 12.2280\n",
            "Epoch 17/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1709.8414 - mae: 14.2922 - val_loss: 266.9479 - val_mae: 11.8601\n",
            "Epoch 18/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1918.8671 - mae: 14.2935 - val_loss: 282.0986 - val_mae: 12.2660\n",
            "Epoch 19/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1989.9257 - mae: 14.1085 - val_loss: 289.6715 - val_mae: 12.6028\n",
            "Epoch 20/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2809.5276 - mae: 14.3075 - val_loss: 277.1875 - val_mae: 12.2939\n",
            "Epoch 21/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1181.7207 - mae: 12.8813 - val_loss: 340.6366 - val_mae: 12.8828\n",
            "Epoch 22/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9375.6631 - mae: 17.7572 - val_loss: 267.8860 - val_mae: 12.0140\n",
            "Epoch 23/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2734.3718 - mae: 13.6489 - val_loss: 276.5997 - val_mae: 11.9884\n",
            "Epoch 24/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2505.0127 - mae: 13.8105 - val_loss: 292.7119 - val_mae: 12.3812\n",
            "Epoch 25/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1673.7744 - mae: 13.0263 - val_loss: 309.5357 - val_mae: 12.4656\n",
            "Epoch 26/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2579.9421 - mae: 14.6031 - val_loss: 272.0019 - val_mae: 11.9803\n",
            "Epoch 27/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4052.1301 - mae: 15.2237 - val_loss: 293.3515 - val_mae: 12.1006\n",
            "Epoch 28/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3877.2932 - mae: 14.4729 - val_loss: 337.7909 - val_mae: 12.8757\n",
            "Epoch 29/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1590.4492 - mae: 13.3819 - val_loss: 349.2091 - val_mae: 13.1805\n",
            "Epoch 30/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3223.1511 - mae: 14.6636 - val_loss: 326.1926 - val_mae: 12.1788\n",
            "Epoch 31/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 870.6010 - mae: 12.7050 - val_loss: 599.2595 - val_mae: 15.8852\n",
            "Epoch 32/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2778.7273 - mae: 15.6938 - val_loss: 329.5924 - val_mae: 12.4026\n",
            "Epoch 33/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 654.8114 - mae: 12.1125 - val_loss: 493.1774 - val_mae: 14.3152\n",
            "Epoch 34/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2397.8340 - mae: 15.3048 - val_loss: 360.5019 - val_mae: 12.9397\n",
            "Epoch 35/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1468.6608 - mae: 13.0074 - val_loss: 338.4463 - val_mae: 12.6788\n",
            "Epoch 36/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1118.3606 - mae: 13.1674 - val_loss: 471.7864 - val_mae: 13.3425\n",
            "Epoch 37/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4099.8911 - mae: 16.1768 - val_loss: 347.5091 - val_mae: 12.7515\n",
            "Epoch 38/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 884.2983 - mae: 12.4097 - val_loss: 511.5334 - val_mae: 13.6208\n",
            "Epoch 39/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2717.9775 - mae: 14.7999 - val_loss: 367.3714 - val_mae: 12.8899\n",
            "Epoch 40/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1242.3846 - mae: 13.1692 - val_loss: 465.9014 - val_mae: 14.5756\n",
            "Epoch 41/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1261.7504 - mae: 13.4330 - val_loss: 530.7581 - val_mae: 14.0789\n",
            "Epoch 42/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 921.5764 - mae: 12.9377 - val_loss: 538.3166 - val_mae: 13.7002\n",
            "Epoch 43/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1488.4336 - mae: 13.8311 - val_loss: 429.2564 - val_mae: 13.7301\n",
            "Epoch 44/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 865.4452 - mae: 12.2906 - val_loss: 572.5192 - val_mae: 14.2906\n",
            "Epoch 45/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1139.0076 - mae: 12.9422 - val_loss: 553.8625 - val_mae: 14.6865\n",
            "Epoch 46/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2009.0360 - mae: 13.2533 - val_loss: 440.9413 - val_mae: 13.4007\n",
            "Epoch 47/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1858.9515 - mae: 13.0925 - val_loss: 425.3334 - val_mae: 13.0237\n",
            "Epoch 48/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1783.9708 - mae: 13.4344 - val_loss: 393.6371 - val_mae: 12.8094\n",
            "Epoch 49/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 624.1364 - mae: 12.0033 - val_loss: 585.6727 - val_mae: 13.8477\n",
            "Epoch 50/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2256.0483 - mae: 13.9048 - val_loss: 347.6844 - val_mae: 12.5947\n",
            "Epoch 51/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 725.5395 - mae: 12.3938 - val_loss: 389.6675 - val_mae: 13.1986\n",
            "Epoch 52/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1219.8019 - mae: 14.4673 - val_loss: 614.5999 - val_mae: 15.3644\n",
            "Epoch 53/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2209.0710 - mae: 14.1145 - val_loss: 436.8051 - val_mae: 12.8799\n",
            "Epoch 54/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 952.3063 - mae: 12.6182 - val_loss: 543.8305 - val_mae: 14.2851\n",
            "Epoch 55/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3418.7776 - mae: 15.1708 - val_loss: 487.2222 - val_mae: 13.1523\n",
            "Epoch 56/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2140.7297 - mae: 13.7693 - val_loss: 515.8043 - val_mae: 13.0074\n",
            "Epoch 57/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1688.4585 - mae: 13.3791 - val_loss: 530.8258 - val_mae: 14.1273\n",
            "Epoch 58/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 734.7482 - mae: 12.5060 - val_loss: 506.5602 - val_mae: 14.1286\n",
            "Epoch 59/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 822.7543 - mae: 13.2861 - val_loss: 568.0685 - val_mae: 13.7259\n",
            "Epoch 60/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2412.6086 - mae: 14.7029 - val_loss: 522.3193 - val_mae: 13.2579\n",
            "Epoch 61/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 770.2250 - mae: 12.1732 - val_loss: 579.4478 - val_mae: 14.0580\n",
            "Epoch 62/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1154.8458 - mae: 13.5981 - val_loss: 417.7984 - val_mae: 13.3719\n",
            "Epoch 63/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 753.5530 - mae: 12.2718 - val_loss: 549.6431 - val_mae: 14.1762\n",
            "Epoch 64/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1080.1559 - mae: 13.1650 - val_loss: 454.1245 - val_mae: 12.8200\n",
            "Epoch 65/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 744.3477 - mae: 12.2195 - val_loss: 442.7937 - val_mae: 13.5689\n",
            "Epoch 66/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1229.4403 - mae: 13.7476 - val_loss: 531.4352 - val_mae: 13.6471\n",
            "Epoch 67/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 965.3837 - mae: 12.2723 - val_loss: 377.5609 - val_mae: 12.6016\n",
            "Epoch 68/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2264.1860 - mae: 13.6072 - val_loss: 476.3977 - val_mae: 13.3019\n",
            "Epoch 69/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2763.0281 - mae: 14.5228 - val_loss: 554.1227 - val_mae: 14.2908\n",
            "Epoch 70/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 778.0316 - mae: 11.7548 - val_loss: 425.3790 - val_mae: 12.7742\n",
            "Epoch 71/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 676.2286 - mae: 11.8726 - val_loss: 549.2023 - val_mae: 13.3009\n",
            "Epoch 72/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1087.3834 - mae: 13.8643 - val_loss: 434.8527 - val_mae: 12.4101\n",
            "Epoch 73/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1616.2917 - mae: 13.0117 - val_loss: 471.7960 - val_mae: 13.0336\n",
            "Epoch 74/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1088.2190 - mae: 12.3583 - val_loss: 498.0468 - val_mae: 13.6086\n",
            "Epoch 75/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 879.8903 - mae: 11.9448 - val_loss: 365.6010 - val_mae: 12.7630\n",
            "Epoch 76/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1306.7084 - mae: 13.0554 - val_loss: 443.5069 - val_mae: 13.5943\n",
            "Epoch 77/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 742.2874 - mae: 12.6811 - val_loss: 427.0896 - val_mae: 12.6346\n",
            "Epoch 78/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1530.4506 - mae: 12.6578 - val_loss: 551.3531 - val_mae: 13.5476\n",
            "Epoch 79/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3588.2307 - mae: 14.7249 - val_loss: 364.5942 - val_mae: 12.5670\n",
            "Epoch 80/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1040.6465 - mae: 12.3911 - val_loss: 502.6725 - val_mae: 12.7739\n",
            "Epoch 81/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1159.0015 - mae: 11.9847 - val_loss: 442.7913 - val_mae: 12.9156\n",
            "Epoch 82/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1252.2888 - mae: 12.8487 - val_loss: 578.5085 - val_mae: 13.7966\n",
            "Epoch 83/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2289.2725 - mae: 14.1455 - val_loss: 375.3258 - val_mae: 12.6376\n",
            "Epoch 84/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 976.2366 - mae: 12.0355 - val_loss: 492.4978 - val_mae: 12.9211\n",
            "Epoch 85/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1213.5400 - mae: 12.5401 - val_loss: 565.5131 - val_mae: 12.8522\n",
            "Epoch 86/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 545.6930 - mae: 11.7481 - val_loss: 430.3635 - val_mae: 13.0998\n",
            "Epoch 87/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1347.3229 - mae: 13.5376 - val_loss: 454.2173 - val_mae: 12.5849\n",
            "Epoch 88/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 738.4420 - mae: 12.0133 - val_loss: 723.4890 - val_mae: 14.9103\n",
            "Epoch 89/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1148.6498 - mae: 13.1266 - val_loss: 576.6022 - val_mae: 13.0852\n",
            "Epoch 90/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1757.8687 - mae: 13.2417 - val_loss: 461.0404 - val_mae: 12.8883\n",
            "Epoch 91/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 667.7315 - mae: 11.9762 - val_loss: 668.6459 - val_mae: 13.8954\n",
            "Epoch 92/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1748.7592 - mae: 14.4650 - val_loss: 526.2631 - val_mae: 12.8912\n",
            "Epoch 93/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1339.4584 - mae: 12.4261 - val_loss: 526.1503 - val_mae: 13.0150\n",
            "Epoch 94/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 567.6615 - mae: 11.2376 - val_loss: 474.8300 - val_mae: 12.2610\n",
            "Epoch 95/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1346.2507 - mae: 12.3525 - val_loss: 564.9974 - val_mae: 13.0580\n",
            "Epoch 96/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 707.4739 - mae: 11.6895 - val_loss: 474.2712 - val_mae: 13.0776\n",
            "Epoch 97/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 611.0797 - mae: 11.6139 - val_loss: 510.7155 - val_mae: 13.3320\n",
            "Epoch 98/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 994.2411 - mae: 12.3100 - val_loss: 455.2288 - val_mae: 12.6899\n",
            "Epoch 99/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 593.4375 - mae: 11.4408 - val_loss: 349.1647 - val_mae: 12.4719\n",
            "Epoch 100/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1224.1117 - mae: 12.4419 - val_loss: 493.0376 - val_mae: 12.3740\n",
            "Epoch 101/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 737.5496 - mae: 11.4671 - val_loss: 424.6638 - val_mae: 12.7518\n",
            "Epoch 102/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 752.5208 - mae: 11.9061 - val_loss: 438.1172 - val_mae: 12.1899\n",
            "Epoch 103/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1317.7833 - mae: 12.9635 - val_loss: 505.8980 - val_mae: 12.8488\n",
            "Epoch 104/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1053.4447 - mae: 12.4110 - val_loss: 485.3212 - val_mae: 12.9029\n",
            "Epoch 105/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1118.1885 - mae: 12.4852 - val_loss: 540.6674 - val_mae: 12.8426\n",
            "Epoch 106/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 648.3086 - mae: 11.7226 - val_loss: 520.9619 - val_mae: 13.0165\n",
            "Epoch 107/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 832.3773 - mae: 12.3923 - val_loss: 372.7641 - val_mae: 11.7251\n",
            "Epoch 108/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1344.0720 - mae: 12.7011 - val_loss: 599.5961 - val_mae: 12.8668\n",
            "Epoch 109/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 762.1563 - mae: 12.0841 - val_loss: 497.4886 - val_mae: 13.1337\n",
            "Epoch 110/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 938.7214 - mae: 13.0569 - val_loss: 437.5419 - val_mae: 12.5691\n",
            "Epoch 111/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 709.8186 - mae: 11.5892 - val_loss: 404.9478 - val_mae: 12.9271\n",
            "Epoch 112/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 541.4341 - mae: 12.0390 - val_loss: 613.8698 - val_mae: 13.3074\n",
            "Epoch 113/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1721.2894 - mae: 13.6546 - val_loss: 391.0084 - val_mae: 11.9536\n",
            "Epoch 114/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 997.9780 - mae: 11.9591 - val_loss: 399.9670 - val_mae: 12.3350\n",
            "Epoch 115/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 812.1271 - mae: 11.5783 - val_loss: 346.6004 - val_mae: 12.3689\n",
            "Epoch 116/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1068.5409 - mae: 13.5000 - val_loss: 398.1341 - val_mae: 12.2717\n",
            "Epoch 117/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 422.9559 - mae: 10.7604 - val_loss: 659.7972 - val_mae: 13.3662\n",
            "Epoch 118/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1545.5153 - mae: 13.5514 - val_loss: 441.9305 - val_mae: 12.4598\n",
            "Epoch 119/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 544.0822 - mae: 10.8680 - val_loss: 411.9208 - val_mae: 11.8787\n",
            "Epoch 120/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 509.0876 - mae: 11.1007 - val_loss: 573.4803 - val_mae: 13.4212\n",
            "Epoch 121/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1046.2285 - mae: 12.1238 - val_loss: 500.9273 - val_mae: 13.2670\n",
            "Epoch 122/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 483.2154 - mae: 11.9248 - val_loss: 468.1963 - val_mae: 12.3010\n",
            "Epoch 123/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 755.2500 - mae: 11.5104 - val_loss: 543.4294 - val_mae: 12.4777\n",
            "Epoch 124/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1085.4351 - mae: 11.5583 - val_loss: 634.7229 - val_mae: 13.1418\n",
            "Epoch 125/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 738.7151 - mae: 11.8380 - val_loss: 430.0685 - val_mae: 11.8408\n",
            "Epoch 126/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2142.0647 - mae: 12.4113 - val_loss: 437.8511 - val_mae: 12.4988\n",
            "Epoch 127/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 781.4672 - mae: 11.4317 - val_loss: 580.1121 - val_mae: 12.9592\n",
            "Epoch 128/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 834.4385 - mae: 12.2230 - val_loss: 572.8947 - val_mae: 12.7935\n",
            "Epoch 129/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1146.5476 - mae: 12.5717 - val_loss: 629.8201 - val_mae: 13.0000\n",
            "Epoch 130/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 593.6320 - mae: 10.9834 - val_loss: 400.5756 - val_mae: 11.8962\n",
            "Epoch 131/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 835.5756 - mae: 11.8568 - val_loss: 471.7359 - val_mae: 12.0328\n",
            "Epoch 132/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1025.2375 - mae: 11.9601 - val_loss: 549.9788 - val_mae: 12.7891\n",
            "Epoch 133/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 615.3361 - mae: 11.9357 - val_loss: 526.2589 - val_mae: 12.6308\n",
            "Epoch 134/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 786.4244 - mae: 11.5814 - val_loss: 560.7435 - val_mae: 12.5964\n",
            "Epoch 135/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 731.7678 - mae: 11.9724 - val_loss: 530.1965 - val_mae: 12.2988\n",
            "Epoch 136/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 758.2648 - mae: 11.6204 - val_loss: 413.6310 - val_mae: 12.2987\n",
            "Epoch 137/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 538.8260 - mae: 11.3350 - val_loss: 554.4554 - val_mae: 13.3660\n",
            "Epoch 138/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 601.2956 - mae: 11.0903 - val_loss: 484.9026 - val_mae: 13.1486\n",
            "Epoch 139/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1116.4657 - mae: 12.5652 - val_loss: 469.1542 - val_mae: 12.7216\n",
            "Epoch 140/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 423.3454 - mae: 11.0286 - val_loss: 476.7048 - val_mae: 12.8099\n",
            "Epoch 141/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 531.7504 - mae: 11.2988 - val_loss: 449.0464 - val_mae: 12.0930\n",
            "Epoch 142/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1013.1614 - mae: 12.0709 - val_loss: 540.1351 - val_mae: 12.4714\n",
            "Epoch 143/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2324.5063 - mae: 13.4603 - val_loss: 528.9629 - val_mae: 12.6645\n",
            "Epoch 144/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 496.9195 - mae: 10.8241 - val_loss: 511.4816 - val_mae: 13.6784\n",
            "Epoch 145/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1276.7444 - mae: 12.5436 - val_loss: 392.5453 - val_mae: 11.8433\n",
            "Epoch 146/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 681.2797 - mae: 10.7370 - val_loss: 429.8438 - val_mae: 12.4175\n",
            "Epoch 147/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 744.3380 - mae: 11.4345 - val_loss: 495.2818 - val_mae: 12.4302\n",
            "Epoch 148/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 595.3893 - mae: 11.4001 - val_loss: 661.0487 - val_mae: 13.8185\n",
            "Epoch 149/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 398.2751 - mae: 10.6364 - val_loss: 561.3853 - val_mae: 12.8836\n",
            "Epoch 150/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 635.5669 - mae: 11.5073 - val_loss: 621.2908 - val_mae: 12.9833\n",
            "Epoch 151/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 902.4316 - mae: 11.6094 - val_loss: 515.8468 - val_mae: 13.0749\n",
            "Epoch 152/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 949.2387 - mae: 11.7071 - val_loss: 389.3989 - val_mae: 12.5212\n",
            "Epoch 153/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 448.5215 - mae: 11.1681 - val_loss: 493.8314 - val_mae: 13.1005\n",
            "Epoch 154/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 419.6204 - mae: 11.1060 - val_loss: 426.1777 - val_mae: 11.8978\n",
            "Epoch 155/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 541.5916 - mae: 11.3096 - val_loss: 576.5079 - val_mae: 13.3965\n",
            "Epoch 156/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 821.9997 - mae: 11.7038 - val_loss: 500.9828 - val_mae: 12.2814\n",
            "Epoch 157/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 484.9755 - mae: 10.8727 - val_loss: 503.9458 - val_mae: 12.0977\n",
            "Epoch 158/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 430.6991 - mae: 11.0311 - val_loss: 590.7080 - val_mae: 13.0617\n",
            "Epoch 159/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 543.2991 - mae: 10.9196 - val_loss: 426.3785 - val_mae: 12.7128\n",
            "Epoch 160/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 448.0649 - mae: 11.1276 - val_loss: 534.8157 - val_mae: 12.5988\n",
            "Epoch 161/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 394.5342 - mae: 10.6231 - val_loss: 629.5937 - val_mae: 12.3380\n",
            "Epoch 162/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 721.8105 - mae: 11.4778 - val_loss: 443.1531 - val_mae: 12.3901\n",
            "Epoch 163/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 435.5060 - mae: 10.7509 - val_loss: 532.0061 - val_mae: 13.0695\n",
            "Epoch 164/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 421.2935 - mae: 10.7965 - val_loss: 530.5011 - val_mae: 12.2466\n",
            "Epoch 165/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 389.8371 - mae: 10.9721 - val_loss: 603.1747 - val_mae: 13.5514\n",
            "Epoch 166/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 382.8457 - mae: 10.9482 - val_loss: 649.8840 - val_mae: 12.9360\n",
            "Epoch 167/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 890.3748 - mae: 11.8113 - val_loss: 620.9236 - val_mae: 12.7309\n",
            "Epoch 168/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 702.3153 - mae: 11.4965 - val_loss: 432.6717 - val_mae: 12.1896\n",
            "Epoch 169/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 415.5362 - mae: 10.6704 - val_loss: 667.5979 - val_mae: 12.8548\n",
            "Epoch 170/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 400.1941 - mae: 10.7290 - val_loss: 343.1521 - val_mae: 11.7461\n",
            "Epoch 171/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 679.0018 - mae: 12.0063 - val_loss: 500.0519 - val_mae: 12.4451\n",
            "Epoch 172/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1469.2994 - mae: 12.0356 - val_loss: 492.7323 - val_mae: 12.8049\n",
            "Epoch 173/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 992.0502 - mae: 12.1697 - val_loss: 365.7173 - val_mae: 12.3901\n",
            "Epoch 174/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 779.8878 - mae: 11.5795 - val_loss: 776.3561 - val_mae: 13.1173\n",
            "Epoch 175/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 445.9790 - mae: 10.8478 - val_loss: 474.3236 - val_mae: 12.0575\n",
            "Epoch 176/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 824.7339 - mae: 11.4744 - val_loss: 519.4285 - val_mae: 12.5450\n",
            "Epoch 177/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 390.0640 - mae: 10.4941 - val_loss: 668.0400 - val_mae: 13.4640\n",
            "Epoch 178/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 460.2584 - mae: 11.6286 - val_loss: 650.9487 - val_mae: 13.0038\n",
            "Epoch 179/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 370.4209 - mae: 10.7363 - val_loss: 563.6959 - val_mae: 12.3454\n",
            "Epoch 180/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 430.9098 - mae: 10.7428 - val_loss: 830.9184 - val_mae: 13.4526\n",
            "Epoch 181/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 938.7526 - mae: 11.9583 - val_loss: 662.3613 - val_mae: 13.4175\n",
            "Epoch 182/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 277.0169 - mae: 10.2092 - val_loss: 684.6497 - val_mae: 14.1610\n",
            "Epoch 183/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 934.4263 - mae: 13.0219 - val_loss: 529.5716 - val_mae: 12.9426\n",
            "Epoch 184/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 481.4257 - mae: 10.7655 - val_loss: 388.3261 - val_mae: 12.7002\n",
            "Epoch 185/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 332.9372 - mae: 10.6530 - val_loss: 620.0170 - val_mae: 12.6828\n",
            "Epoch 186/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 388.8260 - mae: 11.0400 - val_loss: 471.3906 - val_mae: 12.4587\n",
            "Epoch 187/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 445.9254 - mae: 10.8492 - val_loss: 824.3292 - val_mae: 13.5464\n",
            "Epoch 188/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 792.3323 - mae: 11.5268 - val_loss: 697.7735 - val_mae: 13.1198\n",
            "Epoch 189/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 989.6757 - mae: 11.6083 - val_loss: 610.3879 - val_mae: 12.7969\n",
            "Epoch 190/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 394.7020 - mae: 10.8496 - val_loss: 371.4047 - val_mae: 12.0313\n",
            "Epoch 191/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 644.6799 - mae: 10.9988 - val_loss: 520.1808 - val_mae: 12.8221\n",
            "Epoch 192/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 569.2750 - mae: 11.1852 - val_loss: 598.1064 - val_mae: 12.8861\n",
            "Epoch 193/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 383.3227 - mae: 10.8269 - val_loss: 585.8656 - val_mae: 13.0655\n",
            "Epoch 194/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 379.4867 - mae: 10.6928 - val_loss: 486.6212 - val_mae: 12.3277\n",
            "Epoch 195/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 312.8260 - mae: 10.4472 - val_loss: 516.5648 - val_mae: 12.3831\n",
            "Epoch 196/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 302.1315 - mae: 10.3805 - val_loss: 577.7138 - val_mae: 13.0095\n",
            "Epoch 197/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 337.6622 - mae: 10.3181 - val_loss: 351.2296 - val_mae: 12.4041\n",
            "Epoch 198/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 301.0775 - mae: 10.4163 - val_loss: 590.3101 - val_mae: 12.9828\n",
            "Epoch 199/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 344.9518 - mae: 10.5768 - val_loss: 487.2936 - val_mae: 12.3909\n",
            "Epoch 200/200\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 294.2538 - mae: 10.2800 - val_loss: 558.6321 - val_mae: 12.6158\n",
            "MLP Train Loss (RMSE): 18.01160226526528\n",
            "MLP Validation Loss (RMSE): 23.635398877068376, MAE: 12.615777015686035\n",
            "MLP Test Loss (RMSE): 31.68229990769935, MAE: 15.522970199584961\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = processed_data.drop(columns=['TM', 'PM10'])\n",
        "y = processed_data['PM10']\n",
        "\n",
        "# Split into training, validation, and test sets\n",
        "train_size = 0.7\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=train_size, shuffle=False)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# MLP 모델 설계\n",
        "mlp_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "mlp_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# 학습\n",
        "history = mlp_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_data=(X_val_scaled, y_val),\n",
        "    epochs=200,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 평가\n",
        "train_loss = history.history['loss'][-1]  # 마지막 epoch의 train loss\n",
        "val_loss, val_mae = mlp_model.evaluate(X_val_scaled, y_val, verbose=0)\n",
        "test_loss, test_mae = mlp_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "\n",
        "print(f\"MLP Train Loss (RMSE): {np.sqrt(train_loss)}\")\n",
        "print(f\"MLP Validation Loss (RMSE): {np.sqrt(val_loss)}, MAE: {val_mae}\")\n",
        "print(f\"MLP Test Loss (RMSE): {np.sqrt(test_loss)}, MAE: {test_mae}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}