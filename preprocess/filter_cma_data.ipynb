{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중국 Top-5 지역\n",
    "locations = [\"qingdao\", \"dalian\", \"tongliao\", \"yanan\", \"chifeng\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINEDUST_DIR = \"../collect_data/data/cma/finedust\"\n",
    "WEATHER_DIR = \"../collect_data/data/cma/weather\"\n",
    "\n",
    "finedust_files = os.listdir(FINEDUST_DIR)\n",
    "weather_files = os.listdir(WEATHER_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dust = pd.read_csv(os.path.join(FINEDUST_DIR, finedust_files[0]))\n",
    "df_weather = pd.read_csv(os.path.join(WEATHER_DIR, weather_files[0]))\n",
    "\n",
    "dust_columns = df_dust.columns\n",
    "weather_columns = df_weather.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_columns_to_float(df, columns):\n",
    "    \"\"\"\n",
    "    Converts specified columns of a DataFrame to float.\n",
    "    \"\"\"\n",
    "    # Apply pd.to_numeric with errors='coerce' to handle non-numeric values\n",
    "    df[columns] = df[columns].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Explicitly cast columns to float to ensure the correct dtype\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(float)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tm_column(df, column_name):\n",
    "    \"\"\"\n",
    "    Cleans the specified column by removing \".\" and \":\" characters.\n",
    "    \"\"\"\n",
    "    df[column_name] = df[column_name].apply(lambda x: x.replace('.', '').replace(':', ''))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_missing_values(df, missing_values, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Replaces specified missing values with NaN and drops columns with\n",
    "    missing value ratios above the threshold.\n",
    "    \"\"\"\n",
    "    # Replace specified missing values with NaN\n",
    "    df = df.replace(missing_values, np.nan)\n",
    "    \n",
    "    # Calculate the missing value ratio for each column\n",
    "    missing_ratios = df.isnull().mean()\n",
    "    \n",
    "    # Drop columns with missing value ratio above the threshold\n",
    "    columns_to_drop = missing_ratios[missing_ratios > threshold].index\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_and_merge(df1, df2, column1, column2, on, how='inner'):\n",
    "    \"\"\"\n",
    "    Unifies the format of the specified columns in two DataFrames and merges them.\n",
    "\n",
    "    Parameters:\n",
    "    - df1 (pd.DataFrame): The first DataFrame.\n",
    "    - df2 (pd.DataFrame): The second DataFrame.\n",
    "    - column1 (str): The column in df1 to be unified (e.g., 'YYMMDDHHMI').\n",
    "    - column2 (str): The column in df2 to be unified (e.g., 'TM').\n",
    "    - on (list): The list of column names to merge on.\n",
    "    - how (str): The merge method (default: 'inner').\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The merged DataFrame.\n",
    "    \"\"\"\n",
    "    # Rename columns for consistency\n",
    "    df1 = df1.rename(columns={column1: column2})\n",
    "\n",
    "    # Merge the DataFrames\n",
    "    merged_df = pd.merge(df1, df2, on=on, how=how)\n",
    "    merged_df = merged_df.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cma_2018_2022_tongliao.csv\n",
      "cma_2018_2022_tongliao.csv\n",
      "\n",
      "cma_2018_2022_qingdao.csv\n",
      "cma_2018_2022_qingdao.csv\n",
      "\n",
      "cma_2018_2022_dalian.csv\n",
      "cma_2018_2022_dalian.csv\n",
      "\n",
      "cma_2018_2022_yanan.csv\n",
      "cma_2018_2022_yanan.csv\n",
      "\n",
      "cma_2018_2022_chifeng.csv\n",
      "cma_2018_2022_chifeng.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dust_file, weather_file in zip(finedust_files, weather_files):\n",
    "    print(dust_file)\n",
    "    print(weather_file)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chifeng'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dust_file.split(\"_\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 파일 저장\n",
    "\n",
    "FILTER_URL = \"../collect_data/filtered/cma/filtered\"\n",
    "os.makedirs(FILTER_URL, exist_ok=True)\n",
    "\n",
    "for dust_file, weather_file in zip(finedust_files, weather_files):\n",
    "    loc = dust_file.split(\"_\")[-1].split(\".\")[0]\n",
    "\n",
    "    df_dust = pd.read_csv(os.path.join(FINEDUST_DIR, dust_file))\n",
    "    df_weather = pd.read_csv(os.path.join(WEATHER_DIR, weather_file))\n",
    "\n",
    "    df_dust = convert_columns_to_float(df_dust, [\"STN\", \"PM10\"])\n",
    "    df_dust = clean_tm_column(df_dust, \"TM\")\n",
    "\n",
    "    df_weather = convert_columns_to_float(df_weather, weather_columns[1:])\n",
    "    df_weather[\"YYMMDDHHMI\"] = df_weather[\"YYMMDDHHMI\"].astype(str)\n",
    "    df_weather = clean_missing_values(df_weather, [-9, -99, -999])\n",
    "\n",
    "    df_merged = unify_and_merge(df_weather, df_dust, \"YYMMDDHHMI\", \"TM\", [\"TM\", \"STN\"], how='inner')\n",
    "    df_merged.to_csv(f\"{FILTER_URL}/{loc}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TM      0.000000\n",
       "STN     0.000000\n",
       "IW      0.000000\n",
       "IR      0.000000\n",
       "IX      0.000000\n",
       "VV      0.058213\n",
       "WD      0.019404\n",
       "WS      0.067915\n",
       "TA      0.116426\n",
       "TD      0.261958\n",
       "HM      0.174639\n",
       "PS      0.029106\n",
       "PT      0.000000\n",
       "PR      0.019404\n",
       "RH      0.000000\n",
       "ORG     0.000000\n",
       "PM10    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.isnull().mean() * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
