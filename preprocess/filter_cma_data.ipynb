{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중국 Top-5 지역\n",
    "locations = [\"qingdao\", \"dalian\", \"tongliao\", \"yanan\", \"chifeng\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINEDUST_DIR = \"../collect_data/data/cma/finedust\"\n",
    "WEATHER_DIR = \"../collect_data/data/cma/weather\"\n",
    "\n",
    "finedust_files = os.listdir(FINEDUST_DIR)\n",
    "weather_files = os.listdir(WEATHER_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dust = pd.read_csv(os.path.join(FINEDUST_DIR, finedust_files[0]))\n",
    "df_weather = pd.read_csv(os.path.join(WEATHER_DIR, weather_files[0]))\n",
    "\n",
    "dust_columns = df_dust.columns\n",
    "weather_columns = df_weather.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_columns_to_float(df, columns):\n",
    "    \"\"\"\n",
    "    Converts specified columns of a DataFrame to float.\n",
    "    \"\"\"\n",
    "    # Apply pd.to_numeric with errors='coerce' to handle non-numeric values\n",
    "    df[columns] = df[columns].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Explicitly cast columns to float to ensure the correct dtype\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(float)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tm_column(df, column_name):\n",
    "    \"\"\"\n",
    "    Cleans the specified column by removing \".\" and \":\" characters.\n",
    "    \"\"\"\n",
    "    df[column_name] = df[column_name].apply(lambda x: x.replace('.', '').replace(':', ''))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_missing_values(df, missing_values, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Replaces specified missing values with NaN and drops columns with\n",
    "    missing value ratios above the threshold.\n",
    "    \"\"\"\n",
    "    # Replace specified missing values with NaN\n",
    "    df = df.replace(missing_values, np.nan)\n",
    "    \n",
    "    # Calculate the missing value ratio for each column\n",
    "    missing_ratios = df.isnull().mean()\n",
    "    \n",
    "    # Drop columns with missing value ratio above the threshold\n",
    "    columns_to_drop = missing_ratios[missing_ratios > threshold].index\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_and_merge(df1, df2, column1, column2, on, how='inner'):\n",
    "    \"\"\"\n",
    "    Unifies the format of the specified columns in two DataFrames and merges them.\n",
    "\n",
    "    Parameters:\n",
    "    - df1 (pd.DataFrame): The first DataFrame.\n",
    "    - df2 (pd.DataFrame): The second DataFrame.\n",
    "    - column1 (str): The column in df1 to be unified (e.g., 'YYMMDDHHMI').\n",
    "    - column2 (str): The column in df2 to be unified (e.g., 'TM').\n",
    "    - on (list): The list of column names to merge on.\n",
    "    - how (str): The merge method (default: 'inner').\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The merged DataFrame.\n",
    "    \"\"\"\n",
    "    # Rename columns for consistency\n",
    "    df1 = df1.rename(columns={column1: column2})\n",
    "\n",
    "    # Merge the DataFrames\n",
    "    merged_df = pd.merge(df1, df2, on=on, how=how)\n",
    "    merged_df = merged_df.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cma_2018_2022_qingdao.csv\n",
      "cma_2018_2022_qingdao.csv\n",
      "\n",
      "cma_2018_2022_chifeng.csv\n",
      "cma_2018_2022_chifeng.csv\n",
      "\n",
      "cma_2018_2022_tongliao.csv\n",
      "cma_2018_2022_tongliao.csv\n",
      "\n",
      "cma_2018_2022_yanan.csv\n",
      "cma_2018_2022_yanan.csv\n",
      "\n",
      "cma_2018_2022_dalian.csv\n",
      "cma_2018_2022_dalian.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dust_file, weather_file in zip(finedust_files, weather_files):\n",
    "    print(dust_file)\n",
    "    print(weather_file)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dalian'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dust_file.split(\"_\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TM']\n",
      "['TM', 'ORG', 'STN', 'PM10']\n",
      "['TM']\n",
      "['TM', 'ORG', 'STN', 'PM10']\n",
      "['TM']\n",
      "['TM', 'ORG', 'STN', 'PM10']\n",
      "['TM']\n",
      "['TM', 'ORG', 'STN', 'PM10']\n",
      "['TM']\n",
      "['TM', 'ORG', 'STN', 'PM10']\n"
     ]
    }
   ],
   "source": [
    "# Merge 파일 저장\n",
    "\n",
    "FILTER_URL = \"../collect_data/filtered/cma/filtered\"\n",
    "os.makedirs(FILTER_URL, exist_ok=True)\n",
    "\n",
    "for dust_file, weather_file in zip(finedust_files, weather_files):\n",
    "    loc = dust_file.split(\"_\")[-1].split(\".\")[0]\n",
    "\n",
    "    df_dust = pd.read_csv(os.path.join(FINEDUST_DIR, dust_file))\n",
    "    df_weather = pd.read_csv(os.path.join(WEATHER_DIR, weather_file))\n",
    "\n",
    "    df_dust = convert_columns_to_float(df_dust, [\"STN\", \"PM10\"])\n",
    "    df_dust = clean_tm_column(df_dust, \"TM\")\n",
    "\n",
    "    df_weather = convert_columns_to_float(df_weather, weather_columns[1:])\n",
    "    df_weather[\"YYMMDDHHMI\"] = df_weather[\"YYMMDDHHMI\"].astype(str)\n",
    "    df_weather = clean_missing_values(df_weather, [-9, -99, -999])\n",
    "\n",
    "    df_merged = unify_and_merge(df_weather, df_dust, \"YYMMDDHHMI\", \"TM\", [\"TM\", \"TM\"], how='inner')\n",
    "    df_merged.to_csv(f\"{FILTER_URL}/{loc}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TM     NaN\n",
       "ORG    NaN\n",
       "STN    NaN\n",
       "PM10   NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.isnull().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### china_loc.txt to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일이 '../collect_data/china_loc.csv'에 저장되었습니다.\n",
      "      STN             TM_ED             TM_ST STN_KO         STN_EN STN_SP  \\\n",
      "0   52203  2100.12.31.00:00  2006.12.31.23:00     하미           Hami      2   \n",
      "1   52418  2100.12.31.00:00  2006.12.31.23:00     둔황       Dunhuang      2   \n",
      "2   53068  2100.12.31.00:00  2007.03.28.23:00  얼렌하오터        Erenhot      3   \n",
      "3   53276  2100.12.31.00:00  2005.03.08.00:00    쥐르허         Jurihe      1   \n",
      "4   53336  2100.12.31.00:00  2006.12.31.23:00  우라터중치  Wulatezhongqi      2   \n",
      "5   53543  2100.12.31.00:00  2006.12.31.23:00     둥성      Dongsheng      2   \n",
      "6   53787  2100.12.31.00:00  2005.03.08.00:00     위서          Yushe      1   \n",
      "7   53845  2100.12.31.00:00  2006.12.31.22:00     야난          Yanan      2   \n",
      "8   54135  2100.12.31.00:00  2005.03.08.00:00    통랴오       Tongliao      1   \n",
      "9   54157  2100.12.31.00:00  2007.03.28.23:00     쓰핑         Siping      3   \n",
      "10  54218  2100.12.31.00:00  2007.03.28.23:00     츠펑        Chifeng      3   \n",
      "11  54497  2100.12.31.00:00  2007.03.28.23:00     단둥        Dandong      3   \n",
      "12  54662  2100.12.31.00:00  2005.03.08.00:00     다렌         Dalian      1   \n",
      "13  54725  2100.12.31.00:00  2005.03.08.00:00    후이민         Huimin      1   \n",
      "14  54857  2100.12.31.00:00  2007.03.28.23:00    칭다오        Qingdao      3   \n",
      "\n",
      "             LON          LAT       HT  \n",
      "0    93.52000000  42.82000000   737.20  \n",
      "1    94.68000000  40.15000000   100.00  \n",
      "2   112.00000000  43.39000000     0.00  \n",
      "3   112.90000000  42.40000000  1150.80  \n",
      "4   108.52000000  41.57000000  1288.00  \n",
      "5   109.98000000  39.83000000  1461.90  \n",
      "6   112.98000000  37.07000000  1041.40  \n",
      "7   109.50000000  36.60000000   958.50  \n",
      "8   122.37000000  43.60000000   178.50  \n",
      "9   124.20000000  43.11000000     0.00  \n",
      "10  118.93000000  42.27000000     0.00  \n",
      "11  124.20000000  40.03000000     0.00  \n",
      "12  121.63000000  38.90000000    91.50  \n",
      "13  117.53000000  37.48000000    11.70  \n",
      "14  120.30000000  36.07000000     0.00  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 경로 설정\n",
    "input_file = '../collect_data/china_loc.txt'\n",
    "output_file = '../collect_data/china_loc.csv'\n",
    "\n",
    "# 데이터 읽기\n",
    "with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "    lines = infile.readlines()\n",
    "\n",
    "# 데이터 정리\n",
    "data = []\n",
    "for line in lines:\n",
    "    # 주석(#)이나 빈 줄은 건너뜀\n",
    "    if line.startswith('#') or line.strip() == '':\n",
    "        continue\n",
    "    # 공백 기준으로 데이터 나누기\n",
    "    row = line.split()\n",
    "    data.append(row)\n",
    "\n",
    "# DataFrame 생성\n",
    "columns = [\"#\", \"STN\", \"TM_ED\", \"TM_ST\", \"STN_KO\", \"STN_EN\", \"STN_SP\", \"LON\", \"LAT\", \"HT\", \"STN_2\", \"FCT_ID\"]\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df = df.drop(columns=[\"#\"])\n",
    "df = df.drop(columns=[\"FCT_ID\"])\n",
    "df = df.drop(columns=[\"STN_2\"])\n",
    "\n",
    "df = df.iloc[2:].reset_index(drop=True)\n",
    "df = df.iloc[:-1]\n",
    "\n",
    "# CSV 파일로 저장\n",
    "df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"CSV 파일이 '{output_file}'에 저장되었습니다.\")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path = '../collect_data/china_loc.csv'\n",
    "df_meta = pd.read_csv(meta_path)\n",
    "\n",
    "locations = [\"qingdao\", \"dalian\", \"tongliao\", \"yanan\", \"chifeng\"]\n",
    "BASE_PATH = \"../collect_data/filtered/cma\"\n",
    "\n",
    "for loc in locations:\n",
    "    df_loc = pd.read_csv(f\"{BASE_PATH}/{loc}.csv\")\n",
    "    meta_subset = df_meta[[\"STN\", \"LON\", \"LAT\"]]\n",
    "\n",
    "    df_loc = pd.merge(df_loc, meta_subset, on=\"STN\", how=\"left\")\n",
    "    df_loc.to_csv(f\"{BASE_PATH}/{loc}_meta.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
